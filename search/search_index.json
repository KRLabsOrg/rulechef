{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RuleChef","text":"<p>Learn rule-based models from examples using LLM-powered synthesis.</p> <p>RuleChef learns regex, Python code, and spaCy patterns from labeled examples using LLM-powered synthesis. You provide examples, RuleChef generates rules, and those rules run locally without any LLM at inference time.</p>"},{"location":"#why-rules-instead-of-llms","title":"Why Rules Instead of LLMs?","text":"LLM Inference RuleChef Rules Cost ~$0.01 per call Free (no API calls) Latency 200-2000ms &lt;0.1ms Determinism Varies between calls Same input = same output Inspectability Black box Readable, editable rules Drift Model updates change behavior Rules don't change unless you change them"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code>flowchart LR\n    A[Examples] --&gt; B[RuleChef + LLM]\n    B --&gt; C[Learned Rules]\n    C --&gt; D[Fast Local Execution]\n    D --&gt; E[Structured Output]</code></pre> <ol> <li>Provide examples \u2014 labeled input/output pairs</li> <li>RuleChef synthesizes rules \u2014 using LLM-powered generation</li> <li>Rules run locally \u2014 regex, code, or spaCy patterns with zero LLM cost</li> </ol>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from openai import OpenAI\nfrom rulechef import RuleChef, Task, TaskType\n\nclient = OpenAI()\ntask = Task(\n    name=\"Year Extraction\",\n    description=\"Extract year spans from text\",\n    input_schema={\"text\": \"str\"},\n    output_schema={\"spans\": \"List[Span]\"},\n    type=TaskType.EXTRACTION,\n)\n\nchef = RuleChef(task, client)\nchef.add_example(\n    {\"text\": \"Built in 1991\"},\n    {\"spans\": [{\"text\": \"1991\", \"start\": 9, \"end\": 13}]}\n)\nchef.learn_rules()\n\nresult = chef.extract({\"text\": \"Founded in 1997\"})\n# {\"spans\": [{\"text\": \"1997\", \"start\": 11, \"end\": 15}]}\n</code></pre>"},{"location":"#what-it-supports","title":"What It Supports","text":"<ul> <li>Extraction \u2014 find text spans</li> <li>NER \u2014 typed entities with schema enforcement</li> <li>Classification \u2014 single-label categorization</li> <li>Transformation \u2014 structured field extraction</li> </ul> <p>Rules can be regex, Python code, or spaCy token/dependency patterns.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation \u2014 install RuleChef and extras</li> <li>How It Works \u2014 architecture, buffer, rules, schemas, coordinators</li> <li>Quick Start \u2014 full examples for all task types</li> <li>Benchmarks \u2014 Banking77 classification results</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":""},{"location":"benchmarks/#banking77-intent-classification","title":"Banking77 Intent Classification","text":"<p>To measure how well RuleChef performs on a real task, we benchmarked on a subset of the Banking77 intent classification dataset \u2014 77 banking customer service intent classes with ~13K examples.</p>"},{"location":"benchmarks/#setup","title":"Setup","text":"<ul> <li>5 classes pinned: <code>beneficiary_not_allowed</code>, <code>card_arrival</code>, <code>disposable_card_limits</code>, <code>exchange_rate</code>, <code>pending_cash_withdrawal</code></li> <li>5-shot per class (25 training examples total)</li> <li>Dev set: remaining ~660 unused training examples (for refinement)</li> <li>Test set: 200 held-out examples from the official test split (never seen during learning)</li> <li>Regex-only rules (no code, no spaCy)</li> <li>Agentic coordinator guiding 15 refinement iterations</li> <li>Model: Kimi K2 via Groq API</li> </ul>"},{"location":"benchmarks/#results-on-held-out-test-set","title":"Results on Held-Out Test Set","text":"Metric Value Accuracy (exact match) 60.5% Micro Precision 100% Micro Recall 60.5% Micro F1 75.4% Macro F1 71.7% Coverage 60.5% (121/200) Rules learned 108 Learning time ~144s Per-query latency 0.19ms"},{"location":"benchmarks/#per-class-breakdown","title":"Per-Class Breakdown","text":"Class Precision Recall F1 exchange_rate 100% 95% 97% pending_cash_withdrawal 100% 82% 90% card_arrival 100% 62% 77% disposable_card_limits 100% 40% 57% beneficiary_not_allowed 100% 22% 37%"},{"location":"benchmarks/#sample-rules","title":"Sample Rules","text":"<p>Here are a few of the 108 regex rules RuleChef learned (full set in <code>benchmarks/results_banking77.json</code>):</p> <pre><code>exchange_rate_keywords       (?i)\\bexchange\\s+rates?\\b\ntrack_card_delivery          (?i)\\b(?:track|delivery|status|arrival|come|received).*\\bcard\\b\ncash_withdrawal_pending      (?i)\\b(?:cash|withdrawal|atm).*\\b(?:pending|still|waiting)\\b\ndisposable_limit_keywords    (?i)\\bdisposable\\s+cards?\\b(?=.*\\b(?:maximum|limit|how many)\\b)\nbeneficiary_ultra_broad      (?i)\\bbeneficiar(?:y|ies)\\b.*\\b(?:not allowed|fail|denied|can't)\\b\n</code></pre>"},{"location":"benchmarks/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Precision is perfect \u2014 zero false positives across all classes. In production, wrong answers are worse than no answer, and rules never give wrong answers.</p> </li> <li> <p>Recall scales with complexity. Simple keyword patterns (<code>exchange_rate</code> at 95%) are easy; nuanced paraphrases (<code>beneficiary_not_allowed</code> at 22%) need more examples or refinement iterations.</p> </li> <li> <p>Zero runtime cost. After learning, every query is a regex match \u2014 no API calls, no tokens, no latency. At 0.19ms per query, you can process ~5K queries per second on a single CPU.</p> </li> <li> <p>The agentic coordinator matters. Without it (simple heuristic coordinator, 3 iterations), accuracy drops to ~49% and Macro F1 to ~60%. The coordinator's per-class guidance lifts Macro F1 from ~60% to 71.7%.</p> </li> </ol>"},{"location":"benchmarks/#reproduce","title":"Reproduce","text":"<pre><code>pip install rulechef[benchmark]\npython benchmarks/benchmark_banking77.py \\\n    --classes beneficiary_not_allowed,card_arrival,disposable_card_limits,exchange_rate,pending_cash_withdrawal \\\n    --shots 5 --max-iterations 15 --agentic \\\n    --base-url https://api.groq.com/openai/v1 \\\n    --model moonshotai/kimi-k2-instruct-0905\n</code></pre>"},{"location":"api/coordinator/","title":"Coordinator","text":"<p>Coordinator protocol and implementations for learning decision logic.</p>"},{"location":"api/coordinator/#coordinatorprotocol","title":"CoordinatorProtocol","text":""},{"location":"api/coordinator/#rulechef.coordinator.CoordinatorProtocol","title":"<code>CoordinatorProtocol</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract interface for learning coordination.</p> <p>Implementations can be simple (heuristics) or agentic (LLM-powered). RuleChef uses this interface, making coordinators swappable.</p>"},{"location":"api/coordinator/#rulechef.coordinator.CoordinatorProtocol.should_trigger_learning","title":"<code>should_trigger_learning(buffer, current_rules)</code>  <code>abstractmethod</code>","text":"<p>Decide if learning should be triggered now.</p> <p>Parameters:</p> Name Type Description Default <code>buffer</code> <code>ExampleBuffer</code> <p>Current example buffer</p> required <code>current_rules</code> <code>list[Rule] | None</code> <p>Currently learned rules (None if first learn)</p> required <p>Returns:</p> Type Description <code>CoordinationDecision</code> <p>CoordinationDecision with should_learn, strategy, reasoning</p>"},{"location":"api/coordinator/#rulechef.coordinator.CoordinatorProtocol.analyze_buffer","title":"<code>analyze_buffer(buffer)</code>  <code>abstractmethod</code>","text":"<p>Analyze current buffer state.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with buffer statistics and insights</p>"},{"location":"api/coordinator/#rulechef.coordinator.CoordinatorProtocol.on_learning_complete","title":"<code>on_learning_complete(old_rules, new_rules, metrics)</code>  <code>abstractmethod</code>","text":"<p>Callback after learning completes.</p> <p>Parameters:</p> Name Type Description Default <code>old_rules</code> <code>list[Rule] | None</code> <p>Rules before learning (None if first learn)</p> required <code>new_rules</code> <code>list[Rule]</code> <p>Newly learned rules</p> required <code>metrics</code> <code>dict[str, Any]</code> <p>Learning metrics (accuracy, etc.)</p> required"},{"location":"api/coordinator/#rulechef.coordinator.CoordinatorProtocol.guide_refinement","title":"<code>guide_refinement(eval_result, iteration, max_iterations)</code>","text":"<p>Analyze per-class metrics and return (guidance_text, should_continue).</p> <p>Called after each refinement iteration. The guidance string is injected into the patch prompt. should_continue=False stops the loop early.</p> <p>Default: no guidance, always continue.</p>"},{"location":"api/coordinator/#rulechef.coordinator.CoordinatorProtocol.audit_rules","title":"<code>audit_rules(rules, rule_metrics)</code>","text":"<p>Audit rules for redundancy, dead rules, and conflicts.</p> <p>Called after learning completes when pruning is enabled. Returns an AuditResult with actions (remove/merge). The engine applies actions and reverts if performance drops.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[Rule]</code> <p>Current learned rules.</p> required <code>rule_metrics</code> <code>list[Any]</code> <p>Per-rule RuleMetrics from evaluate_rules_individually.</p> required <p>Returns:</p> Type Description <code>AuditResult</code> <p>AuditResult with actions to take.</p>"},{"location":"api/coordinator/#simplecoordinator","title":"SimpleCoordinator","text":""},{"location":"api/coordinator/#rulechef.coordinator.SimpleCoordinator","title":"<code>SimpleCoordinator(trigger_threshold=50, correction_threshold=10, verbose=True)</code>","text":"<p>               Bases: <code>CoordinatorProtocol</code></p> <p>Deterministic heuristic-based coordinator.</p> <p>Uses simple rules to make decisions: - First learn: trigger after N examples - Subsequent: trigger after N examples OR M corrections - Strategy selection: corrections_first if corrections, else balanced/diversity</p> <p>Parameters:</p> Name Type Description Default <code>trigger_threshold</code> <code>int</code> <p>Number of examples needed to trigger learning</p> <code>50</code> <code>correction_threshold</code> <code>int</code> <p>Number of corrections to trigger early learning</p> <code>10</code> <code>verbose</code> <code>bool</code> <p>Print coordination decisions</p> <code>True</code>"},{"location":"api/coordinator/#rulechef.coordinator.SimpleCoordinator.should_trigger_learning","title":"<code>should_trigger_learning(buffer, current_rules)</code>","text":"<p>Simple heuristic decision</p>"},{"location":"api/coordinator/#rulechef.coordinator.SimpleCoordinator.analyze_buffer","title":"<code>analyze_buffer(buffer)</code>","text":"<p>Basic buffer statistics</p>"},{"location":"api/coordinator/#rulechef.coordinator.SimpleCoordinator.on_learning_complete","title":"<code>on_learning_complete(old_rules, new_rules, metrics)</code>","text":"<p>Log learning results. metrics is an EvalResult or None.</p>"},{"location":"api/coordinator/#coordinationdecision","title":"CoordinationDecision","text":""},{"location":"api/coordinator/#rulechef.coordinator.CoordinationDecision","title":"<code>CoordinationDecision(should_learn, strategy, reasoning, max_iterations=3, metadata=None)</code>  <code>dataclass</code>","text":"<p>Result of coordinator analysis - explains what/why/how to learn</p>"},{"location":"api/coordinator/#agenticcoordinator","title":"AgenticCoordinator","text":""},{"location":"api/coordinator/#rulechef.coordinator.AgenticCoordinator","title":"<code>AgenticCoordinator(llm_client, model='gpt-4o-mini', min_batch_size=5, min_correction_batch=1, verbose=True, prune_after_learn=False, training_logger=None)</code>","text":"<p>               Bases: <code>CoordinatorProtocol</code></p> <p>LLM-based intelligent coordinator.</p> <p>Uses LLM to make adaptive decisions: - Analyze buffer patterns to detect when learning would be beneficial - Choose optimal sampling strategy based on data characteristics - Decide iteration count based on learning progress - Provide detailed reasoning for decisions</p> <p>Parameters:</p> Name Type Description Default <code>llm_client</code> <code>Any</code> <p>OpenAI client</p> required <code>model</code> <code>str</code> <p>Model to use for coordination</p> <code>'gpt-4o-mini'</code> <code>min_batch_size</code> <code>int</code> <p>Minimum new examples before asking LLM</p> <code>5</code> <code>min_correction_batch</code> <code>int</code> <p>Minimum corrections before asking LLM</p> <code>1</code> <code>verbose</code> <code>bool</code> <p>Print coordination decisions</p> <code>True</code> <code>prune_after_learn</code> <code>bool</code> <p>If True, audit and prune/merge rules after learning</p> <code>False</code> <code>training_logger</code> <p>Optional TrainingDataLogger for capturing LLM calls.</p> <code>None</code>"},{"location":"api/coordinator/#rulechef.coordinator.AgenticCoordinator.should_trigger_learning","title":"<code>should_trigger_learning(buffer, current_rules)</code>","text":"<p>Agentic decision based on buffer content</p>"},{"location":"api/coordinator/#rulechef.coordinator.AgenticCoordinator.analyze_buffer","title":"<code>analyze_buffer(buffer)</code>","text":"<p>Analyze buffer stats</p>"},{"location":"api/coordinator/#rulechef.coordinator.AgenticCoordinator.guide_refinement","title":"<code>guide_refinement(eval_result, iteration, max_iterations)</code>","text":"<p>LLM-powered refinement guidance based on per-class metrics.</p>"},{"location":"api/coordinator/#rulechef.coordinator.AgenticCoordinator.on_learning_complete","title":"<code>on_learning_complete(old_rules, new_rules, metrics)</code>","text":"<p>Log learning results. metrics is an EvalResult or None.</p>"},{"location":"api/coordinator/#rulechef.coordinator.AgenticCoordinator.audit_rules","title":"<code>audit_rules(rules, rule_metrics)</code>","text":"<p>LLM-powered rule audit: merge redundant rules, remove pure noise.</p>"},{"location":"api/coordinator/#auditresult","title":"AuditResult","text":""},{"location":"api/coordinator/#rulechef.coordinator.AuditResult","title":"<code>AuditResult(actions=list(), analysis='')</code>  <code>dataclass</code>","text":"<p>Result of a rule audit.</p>"},{"location":"api/coordinator/#auditaction","title":"AuditAction","text":""},{"location":"api/coordinator/#rulechef.coordinator.AuditAction","title":"<code>AuditAction(action, rule_ids, reason, merged_pattern=None, merged_name=None)</code>  <code>dataclass</code>","text":"<p>A single audit action: remove or merge.</p>"},{"location":"api/core/","title":"Core Types","text":"<p>Data structures used throughout RuleChef.</p>"},{"location":"api/core/#tasktype","title":"TaskType","text":""},{"location":"api/core/#rulechef.core.TaskType","title":"<code>TaskType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Type of task being performed.</p> <p>EXTRACTION:      Find text spans (untyped). Output: {\"spans\": [{\"text\", \"start\", \"end\"}]} NER:             Find typed entities.       Output: {\"entities\": [{\"text\", \"start\", \"end\", \"type\"}]} CLASSIFICATION:  Classify input into a label. Output: {\"label\": \"class_name\"} TRANSFORMATION:  Extract structured data to a custom output schema you define.                  Like GLiNER \u2014 define {\"company\": \"str\", \"amount\": \"str\"} and get exactly that.</p>"},{"location":"api/core/#ruleformat","title":"RuleFormat","text":""},{"location":"api/core/#rulechef.core.RuleFormat","title":"<code>RuleFormat</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Rule representation formats</p>"},{"location":"api/core/#task","title":"Task","text":""},{"location":"api/core/#rulechef.core.Task","title":"<code>Task(name, description, input_schema, output_schema, type=TaskType.EXTRACTION, output_matcher=None, matching_mode='text', text_field=None)</code>  <code>dataclass</code>","text":"<p>Abstract task definition. Describes what we're trying to accomplish.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Task name</p> <code>description</code> <code>str</code> <p>Free text description</p> <code>input_schema</code> <code>dict[str, str]</code> <p>Dict describing input fields</p> <code>output_schema</code> <code>OutputSchema</code> <p>Dict or Pydantic model describing output fields. - Dict: Simple string descriptions (e.g., {\"spans\": \"List[Span]\"}) - Pydantic model: Full type validation with Literal labels</p> <code>type</code> <code>TaskType</code> <p>TaskType enum (EXTRACTION, NER, CLASSIFICATION, TRANSFORMATION)</p> <code>output_matcher</code> <code>OutputMatcher | None</code> <p>Optional custom function to compare outputs.            Signature: (expected: Dict, actual: Dict) -&gt; bool            If not provided, uses default matcher for the task type.</p> <code>matching_mode</code> <code>Literal['text', 'exact']</code> <p>For extraction tasks, choose \"text\" (default) or \"exact\"            to control how span matches are evaluated.</p> <code>text_field</code> <code>str | None</code> <p>Optional input key to use for regex/spaCy matching. If not set,         the longest string field is used.</p>"},{"location":"api/core/#rulechef.core.Task.get_labels","title":"<code>get_labels(field_name='type')</code>","text":"<p>Get label values from output schema.</p> <p>For Pydantic schemas, extracts Literal values from the specified field. For dict schemas, returns empty list (labels not defined).</p>"},{"location":"api/core/#rulechef.core.Task.validate_output","title":"<code>validate_output(output)</code>","text":"<p>Validate output against schema.</p> <p>For Pydantic schemas, uses model validation. For dict schemas, returns (True, []) - no validation.</p> <p>Returns:</p> Type Description <code>tuple[bool, list[str]]</code> <p>Tuple of (is_valid, list_of_error_messages)</p>"},{"location":"api/core/#rulechef.core.Task.get_schema_for_prompt","title":"<code>get_schema_for_prompt()</code>","text":"<p>Render schema for inclusion in LLM prompts.</p> <p>For Pydantic schemas, generates a readable representation with descriptions. For dict schemas, returns the dict as a string.</p>"},{"location":"api/core/#rulechef.core.Task.to_dict","title":"<code>to_dict()</code>","text":""},{"location":"api/core/#rule","title":"Rule","text":""},{"location":"api/core/#rulechef.core.Rule","title":"<code>Rule(id, name, description, format, content, priority=5, confidence=0.5, times_applied=0, successes=0, failures=0, created_at=datetime.now(), output_template=None, output_key=None)</code>  <code>dataclass</code>","text":"<p>Learned extraction rule.</p> <p>For schema-aware rules (NER, TRANSFORMATION), use output_template and output_key to control how matches are mapped to structured output. For legacy rules (EXTRACTION), content holds the pattern directly.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier.</p> <code>name</code> <code>str</code> <p>Human-readable rule name (used for merge-by-name in patching).</p> <code>description</code> <code>str</code> <p>What this rule matches or does.</p> <code>format</code> <code>RuleFormat</code> <p>Rule format (REGEX, CODE, or SPACY).</p> <code>content</code> <code>str</code> <p>Pattern string (regex, code, or JSON-encoded spaCy pattern). Also accessible via the <code>pattern</code> property.</p> <code>priority</code> <code>int</code> <p>Execution priority (1-10, higher runs first).</p> <code>confidence</code> <code>float</code> <p>Confidence score (0.0-1.0), adjusted based on success rate.</p> <code>times_applied</code> <code>int</code> <p>Total number of times this rule has been applied.</p> <code>successes</code> <code>int</code> <p>Number of successful applications.</p> <code>failures</code> <code>int</code> <p>Number of failed applications.</p> <code>created_at</code> <code>datetime</code> <p>When the rule was created.</p> <code>output_template</code> <code>dict[str, Any] | None</code> <p>JSON template for each match, using variables like $0, $1, $start, $end, $ent_type. None for plain span extraction.</p> <code>output_key</code> <code>str | None</code> <p>Which key in the output dict to populate (e.g. 'entities'). Inferred from task type if not set.</p>"},{"location":"api/core/#rulechef.core.Rule.pattern","title":"<code>pattern</code>  <code>property</code> <code>writable</code>","text":"<p>Alias for content - clearer semantics for regex/spaCy patterns</p>"},{"location":"api/core/#rulechef.core.Rule.update_stats","title":"<code>update_stats(success)</code>","text":"<p>Update performance stats and adjust confidence</p>"},{"location":"api/core/#rulechef.core.Rule.to_dict","title":"<code>to_dict()</code>","text":""},{"location":"api/core/#span","title":"Span","text":""},{"location":"api/core/#rulechef.core.Span","title":"<code>Span(text, start, end, score=1.0)</code>  <code>dataclass</code>","text":"<p>A text span with character-level position information.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>The matched text content.</p> <code>start</code> <code>int</code> <p>Start character offset (inclusive) in the source string.</p> <code>end</code> <code>int</code> <p>End character offset (exclusive) in the source string.</p> <code>score</code> <code>float</code> <p>Confidence score for the match, between 0.0 and 1.0.</p>"},{"location":"api/core/#rulechef.core.Span.overlaps","title":"<code>overlaps(other)</code>","text":"<p>Check if spans overlap</p>"},{"location":"api/core/#rulechef.core.Span.overlap_ratio","title":"<code>overlap_ratio(other)</code>","text":"<p>Calculate overlap ratio (IoU)</p>"},{"location":"api/core/#rulechef.core.Span.to_dict","title":"<code>to_dict()</code>","text":""},{"location":"api/core/#dataset","title":"Dataset","text":""},{"location":"api/core/#rulechef.core.Dataset","title":"<code>Dataset(name, task, description='', examples=list(), corrections=list(), feedback=list(), structured_feedback=list(), rules=list(), version=1)</code>  <code>dataclass</code>","text":"<p>Complete training dataset containing examples, corrections, feedback, and rules.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Dataset name, used as the persistence filename.</p> <code>task</code> <code>Task</code> <p>Task definition describing the extraction/classification goal.</p> <code>description</code> <code>str</code> <p>Optional human-readable description.</p> <code>examples</code> <code>list[Example]</code> <p>List of labeled training examples.</p> <code>corrections</code> <code>list[Correction]</code> <p>List of user corrections (highest-value training signal).</p> <code>feedback</code> <code>list[str]</code> <p>Legacy list of plain-text feedback strings (task-level only).</p> <code>structured_feedback</code> <code>list[Feedback]</code> <p>Structured feedback entries at task/example/rule level.</p> <code>rules</code> <code>list[Rule]</code> <p>Learned rules (populated by learn_rules).</p> <code>version</code> <code>int</code> <p>Dataset schema version for forward compatibility.</p>"},{"location":"api/core/#rulechef.core.Dataset.get_all_training_data","title":"<code>get_all_training_data()</code>","text":"<p>Get all examples and corrections combined</p>"},{"location":"api/core/#rulechef.core.Dataset.get_feedback_for","title":"<code>get_feedback_for(level, target_id='')</code>","text":"<p>Get feedback filtered by level and optional target.</p>"},{"location":"api/core/#rulechef.core.Dataset.to_dict","title":"<code>to_dict()</code>","text":""},{"location":"api/core/#example","title":"Example","text":""},{"location":"api/core/#rulechef.core.Example","title":"<code>Example(id, input, expected_output, source, confidence=0.8, timestamp=datetime.now())</code>  <code>dataclass</code>","text":"<p>Regular training example. Lower priority than corrections.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier.</p> <code>input</code> <code>dict[str, Any]</code> <p>Input data dict matching the task's input_schema.</p> <code>expected_output</code> <code>dict[str, Any]</code> <p>Expected output dict matching the task's output_schema.</p> <code>source</code> <code>str</code> <p>Origin of the example ('human_labeled' or 'llm_generated').</p> <code>confidence</code> <code>float</code> <p>Confidence score for this example (0.0-1.0).</p> <code>timestamp</code> <code>datetime</code> <p>When the example was created.</p>"},{"location":"api/core/#correction","title":"Correction","text":""},{"location":"api/core/#rulechef.core.Correction","title":"<code>Correction(id, input, model_output, expected_output, feedback=None, timestamp=datetime.now())</code>  <code>dataclass</code>","text":"<p>User correction -- the highest value training signal.</p> <p>Contains both the wrong output and the correct output so the learner can understand what to fix.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier.</p> <code>input</code> <code>dict[str, Any]</code> <p>Input data dict that was processed.</p> <code>model_output</code> <code>dict[str, Any]</code> <p>The incorrect output that was produced.</p> <code>expected_output</code> <code>dict[str, Any]</code> <p>The correct output the model should have produced.</p> <code>feedback</code> <code>str | None</code> <p>Optional free-text explanation of what went wrong.</p> <code>timestamp</code> <code>datetime</code> <p>When the correction was created.</p>"},{"location":"api/core/#feedback","title":"Feedback","text":""},{"location":"api/core/#rulechef.core.Feedback","title":"<code>Feedback(id, text, level, target_id='', timestamp=datetime.now())</code>  <code>dataclass</code>","text":"<p>User feedback at any level: task, example, or rule.</p> <ul> <li>task: general guidance (\"drugs usually follow dosage like 'mg'\")</li> <li>example: feedback on a specific training item</li> <li>rule: feedback on a specific rule (\"too broad\", \"too specific\")</li> </ul> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Unique identifier.</p> <code>text</code> <code>str</code> <p>The feedback text.</p> <code>level</code> <code>str</code> <p>Feedback scope -- 'task', 'example', or 'rule'.</p> <code>target_id</code> <code>str</code> <p>Empty for task-level; example_id or rule_id otherwise.</p> <code>timestamp</code> <code>datetime</code> <p>When the feedback was created.</p>"},{"location":"api/evaluation/","title":"Evaluation","text":"<p>Evaluation functions and result types.</p>"},{"location":"api/evaluation/#evalresult","title":"EvalResult","text":""},{"location":"api/evaluation/#rulechef.evaluation.EvalResult","title":"<code>EvalResult(micro_precision=0.0, micro_recall=0.0, micro_f1=0.0, macro_f1=0.0, per_class=list(), exact_match=0.0, total_tp=0, total_fp=0, total_fn=0, total_docs=0, failures=list())</code>  <code>dataclass</code>","text":"<p>Rich evaluation result across a dataset.</p> <p>Attributes:</p> Name Type Description <code>micro_precision</code> <code>float</code> <p>Entity-level micro-averaged precision.</p> <code>micro_recall</code> <code>float</code> <p>Entity-level micro-averaged recall.</p> <code>micro_f1</code> <code>float</code> <p>Entity-level micro-averaged F1 score.</p> <code>macro_f1</code> <code>float</code> <p>Macro F1 (unweighted average of per-class F1 scores).</p> <code>per_class</code> <code>list[ClassMetrics]</code> <p>Per-class precision/recall/F1 breakdown.</p> <code>exact_match</code> <code>float</code> <p>Fraction of documents with perfect output (0.0-1.0).</p> <code>total_tp</code> <code>int</code> <p>Total true positive count across all classes.</p> <code>total_fp</code> <code>int</code> <p>Total false positive count across all classes.</p> <code>total_fn</code> <code>int</code> <p>Total false negative count across all classes.</p> <code>total_docs</code> <code>int</code> <p>Number of documents evaluated.</p> <code>failures</code> <code>list[dict]</code> <p>List of failure dicts with keys 'input', 'expected', 'got', 'is_correction'. Used by the refinement loop to generate patches.</p>"},{"location":"api/evaluation/#rulechef.evaluation.EvalResult.to_dict","title":"<code>to_dict()</code>","text":""},{"location":"api/evaluation/#classmetrics","title":"ClassMetrics","text":""},{"location":"api/evaluation/#rulechef.evaluation.ClassMetrics","title":"<code>ClassMetrics(label, tp=0, fp=0, fn=0)</code>  <code>dataclass</code>","text":"<p>Precision / recall / F1 for a single entity type or key.</p> <p>Attributes:</p> Name Type Description <code>label</code> <code>str</code> <p>The class/entity type name.</p> <code>tp</code> <code>int</code> <p>True positive count.</p> <code>fp</code> <code>int</code> <p>False positive count.</p> <code>fn</code> <code>int</code> <p>False negative count.</p>"},{"location":"api/evaluation/#rulechef.evaluation.ClassMetrics.precision","title":"<code>precision</code>  <code>property</code>","text":""},{"location":"api/evaluation/#rulechef.evaluation.ClassMetrics.recall","title":"<code>recall</code>  <code>property</code>","text":""},{"location":"api/evaluation/#rulechef.evaluation.ClassMetrics.f1","title":"<code>f1</code>  <code>property</code>","text":""},{"location":"api/evaluation/#rulechef.evaluation.ClassMetrics.to_dict","title":"<code>to_dict()</code>","text":""},{"location":"api/evaluation/#rulemetrics","title":"RuleMetrics","text":""},{"location":"api/evaluation/#rulechef.evaluation.RuleMetrics","title":"<code>RuleMetrics(rule_id, rule_name, precision=0.0, recall=0.0, f1=0.0, matches=0, true_positives=0, false_positives=0, covered_expected=0, total_expected=0, per_class=list(), sample_matches=list())</code>  <code>dataclass</code>","text":"<p>Evaluation of a single rule in isolation.</p> <p>Attributes:</p> Name Type Description <code>rule_id</code> <code>str</code> <p>Unique identifier of the evaluated rule.</p> <code>rule_name</code> <code>str</code> <p>Human-readable name of the rule.</p> <code>precision</code> <code>float</code> <p>Precision of this rule alone (TP / (TP + FP)).</p> <code>recall</code> <code>float</code> <p>Recall of this rule alone (covered / total expected entities).</p> <code>f1</code> <code>float</code> <p>F1 score derived from precision and recall.</p> <code>matches</code> <code>int</code> <p>Total number of entities this rule produced.</p> <code>true_positives</code> <code>int</code> <p>Entities that matched an expected entity.</p> <code>false_positives</code> <code>int</code> <p>Entities that did not match any expected entity.</p> <code>covered_expected</code> <code>int</code> <p>How many expected entities this rule correctly finds.</p> <code>total_expected</code> <code>int</code> <p>Total expected entities across the full dataset.</p> <code>per_class</code> <code>list[ClassMetrics]</code> <p>Per-class breakdown of TP/FP/FN for this rule.</p> <code>sample_matches</code> <code>list[dict]</code> <p>Up to 10 sample match dicts showing rule behavior.</p>"},{"location":"api/evaluation/#functions","title":"Functions","text":""},{"location":"api/evaluation/#evaluate_dataset","title":"evaluate_dataset","text":""},{"location":"api/evaluation/#rulechef.evaluation.evaluate_dataset","title":"<code>evaluate_dataset(rules, dataset, apply_rules_fn, mode='text')</code>","text":"<p>Evaluate rules against a full dataset, producing entity-level metrics.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[Rule]</code> <p>Rules to evaluate.</p> required <code>dataset</code> <code>Dataset</code> <p>Dataset with examples and corrections.</p> required <code>apply_rules_fn</code> <p>Callable(rules, input_data, task_type, text_field) -&gt; output_dict.</p> required <code>mode</code> <code>str</code> <p>'text' (match by text+type) or 'exact' (match by text+type+start+end).</p> <code>'text'</code> <p>Returns:</p> Type Description <code>EvalResult</code> <p>EvalResult with micro/macro metrics, per-class breakdown, exact match</p> <code>EvalResult</code> <p>rate, and a list of failure dicts for refinement.</p>"},{"location":"api/evaluation/#evaluate_rules_individually","title":"evaluate_rules_individually","text":""},{"location":"api/evaluation/#rulechef.evaluation.evaluate_rules_individually","title":"<code>evaluate_rules_individually(rules, dataset, apply_rules_fn, mode='text', max_samples=10)</code>","text":"<p>Evaluate each rule in isolation against the dataset.</p> <p>For each rule, runs it alone and computes how many expected entities it produces correctly (TP), how many spurious entities it produces (FP), and how many expected entities it misses (recall denominator).</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[Rule]</code> <p>Rules to evaluate individually.</p> required <code>dataset</code> <code>Dataset</code> <p>Dataset with examples and corrections.</p> required <code>apply_rules_fn</code> <p>Callable(rules, input_data, task_type, text_field) -&gt; output_dict.</p> required <code>mode</code> <code>str</code> <p>'text' or 'exact'.</p> <code>'text'</code> <code>max_samples</code> <code>int</code> <p>Max sample matches to store per rule.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[RuleMetrics]</code> <p>List[RuleMetrics], one entry per rule, with per-rule precision/recall/F1,</p> <code>list[RuleMetrics]</code> <p>match counts, per-class breakdown, and sample matches.</p>"},{"location":"api/executor/","title":"Executor","text":"<p>Rule execution engine.</p>"},{"location":"api/executor/#ruleexecutor","title":"RuleExecutor","text":""},{"location":"api/executor/#rulechef.executor.RuleExecutor","title":"<code>RuleExecutor(use_spacy_ner=False)</code>","text":"<p>Executes rules against input data</p> <p>Initialize the rule executor.</p> <p>Parameters:</p> Name Type Description Default <code>use_spacy_ner</code> <code>bool</code> <p>If True, run spaCy's NER pipeline during rule execution so that ENT_TYPE/ENT_ID patterns are available.</p> <code>False</code>"},{"location":"api/executor/#rulechef.executor.RuleExecutor.apply_rules","title":"<code>apply_rules(rules, input_data, task_type=None, text_field=None)</code>","text":"<p>Apply rules to input and return aggregated output.</p> <p>Rules are sorted by priority and applied sequentially. Results are deduplicated by span position. If a rule has no output_key, it is inferred from task_type using DEFAULT_OUTPUT_KEYS.</p> <p>Parameters:</p> Name Type Description Default <code>rules</code> <code>list[Rule]</code> <p>List of rules to apply.</p> required <code>input_data</code> <code>dict</code> <p>Input data dict.</p> required <code>task_type</code> <code>TaskType | None</code> <p>Task type for inferring the default output_key.</p> <code>None</code> <code>text_field</code> <code>str | None</code> <p>Input key to use for regex/spaCy matching.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Output dict with results keyed by output_key (e.g. 'entities',</p> <code>dict</code> <p>'spans', 'label'). Empty dict if no rules matched.</p>"},{"location":"api/executor/#rulechef.executor.RuleExecutor.execute_rule","title":"<code>execute_rule(rule, input_data, text_field=None)</code>","text":"<p>Execute a single rule against input data.</p> <p>Parameters:</p> Name Type Description Default <code>rule</code> <code>Rule</code> <p>The rule to execute.</p> required <code>input_data</code> <code>dict</code> <p>Input data dict.</p> required <code>text_field</code> <code>str | None</code> <p>Input key to use for regex/spaCy matching.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>List of Span or dict results for regex/spaCy rules, or</p> <code>Any</code> <p>arbitrary return value from code rules. Empty list on no match.</p>"},{"location":"api/executor/#functions","title":"Functions","text":""},{"location":"api/executor/#substitute_template","title":"substitute_template","text":""},{"location":"api/executor/#rulechef.executor.substitute_template","title":"<code>substitute_template(template, match_text, start, end, groups=(), ent_type=None, ent_label=None, token_spans=None)</code>","text":"<p>Substitute template variables with actual values from a match.</p> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>dict[str, Any]</code> <p>Output template dict with variable placeholders.</p> required <code>match_text</code> <code>str</code> <p>The full matched text ($0).</p> required <code>start</code> <code>int</code> <p>Start character offset ($start).</p> required <code>end</code> <code>int</code> <p>End character offset ($end).</p> required <code>groups</code> <code>tuple</code> <p>Regex capture groups ($1, $2, ...).</p> <code>()</code> <code>ent_type</code> <code>str | None</code> <p>spaCy entity type string ($ent_type).</p> <code>None</code> <code>ent_label</code> <code>str | None</code> <p>spaCy entity label string ($ent_label).</p> <code>None</code> <code>token_spans</code> <code>list[dict[str, Any]] | None</code> <p>Per-token span dicts for spaCy dependency matches ($1.text, $1.start, $1.end, etc.).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with all template variables replaced by their actual values.</p> <p>Variables: - $0: Full match text - $1, $2, ...: Capture groups - $start: Start character offset - $end: End character offset - $ent_type: Entity type (spaCy only) - $ent_label: Entity label (spaCy only)</p>"},{"location":"api/rulechef/","title":"RuleChef","text":"<p>The main orchestrator class.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef","title":"<code>RuleChef(task=None, client=None, dataset_name='default', storage_path='./rulechef_data', allowed_formats=None, sampling_strategy='balanced', coordinator=None, auto_trigger=False, model='gpt-4o-mini', llm_fallback=False, use_spacy_ner=False, use_grex=True, max_rules=10, max_samples=50, max_rules_per_class=5, max_counter_examples=10, synthesis_strategy='auto', training_logger=None)</code>","text":"<p>Main orchestrator for learning and applying rules</p> <p>Initialize a RuleChef instance.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task | None</code> <p>Task definition describing what to extract/classify. Optional \u2014 if None, use start_observing() and the task will be auto-discovered from observed LLM calls.</p> <code>None</code> <code>client</code> <code>OpenAI | None</code> <p>OpenAI client instance. Creates a default client if not provided.</p> <code>None</code> <code>dataset_name</code> <code>str</code> <p>Name for the dataset, used as the filename for persistence.</p> <code>'default'</code> <code>storage_path</code> <code>str</code> <p>Directory path for saving/loading dataset JSON files.</p> <code>'./rulechef_data'</code> <code>allowed_formats</code> <code>list[RuleFormat] | None</code> <p>Rule formats to allow (e.g. REGEX, CODE, SPACY). Defaults to [REGEX, CODE].</p> <code>None</code> <code>sampling_strategy</code> <code>str</code> <p>How to sample training data for prompts. Options: 'balanced', 'recent', 'diversity', 'uncertain', 'varied'.</p> <code>'balanced'</code> <code>coordinator</code> <code>CoordinatorProtocol | None</code> <p>CoordinatorProtocol implementation for learning decisions. Defaults to SimpleCoordinator.</p> <code>None</code> <code>auto_trigger</code> <code>bool</code> <p>If True, check coordinator after each add_example/add_correction and trigger learning automatically when ready.</p> <code>False</code> <code>model</code> <code>str</code> <p>OpenAI model name for LLM calls.</p> <code>'gpt-4o-mini'</code> <code>llm_fallback</code> <code>bool</code> <p>If True, fall back to direct LLM extraction when rules produce no results.</p> <code>False</code> <code>use_spacy_ner</code> <code>bool</code> <p>If True, enable spaCy NER entity recognition during rule execution (requires spaCy and a model).</p> <code>False</code> <code>use_grex</code> <code>bool</code> <p>If True, use grex for regex pattern suggestion in prompts.</p> <code>True</code> <code>max_rules</code> <code>int</code> <p>Maximum number of rules to generate per synthesis call.</p> <code>10</code> <code>max_samples</code> <code>int</code> <p>Maximum training examples to include in prompts (per-class positives and patch failures are both capped to this).</p> <code>50</code> <code>max_rules_per_class</code> <code>int</code> <p>Maximum rules to generate per class in per-class synthesis.</p> <code>5</code> <code>max_counter_examples</code> <code>int</code> <p>Maximum counter-examples from other classes per prompt.</p> <code>10</code> <code>synthesis_strategy</code> <code>str</code> <p>Strategy for multi-class synthesis. 'auto' uses per-class when multiple classes detected, 'per_class' always uses per-class, any other value uses bulk synthesis.</p> <code>'auto'</code> <code>training_logger</code> <p>Optional TrainingDataLogger instance for capturing all LLM calls as training data for model distillation.</p> <code>None</code>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.add_example","title":"<code>add_example(input_data, output_data, source='human_labeled')</code>","text":"<p>Add a labeled training example.</p> <p>Uses buffer-first architecture: example goes to buffer, then coordinator decides when to trigger learning.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>Input dict matching the task's input_schema.</p> required <code>output_data</code> <code>dict</code> <p>Expected output dict matching the task's output_schema.</p> required <code>source</code> <code>str</code> <p>Origin of the example, e.g. 'human_labeled' or 'llm_generated'.</p> <code>'human_labeled'</code>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.add_correction","title":"<code>add_correction(input_data, model_output, expected_output, feedback=None)</code>","text":"<p>Add a user correction (high value signal).</p> <p>Uses buffer-first architecture: correction goes to buffer, then coordinator decides when to trigger learning. Corrections are high-priority signals.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>Input dict that was processed.</p> required <code>model_output</code> <code>dict</code> <p>The incorrect output that was produced.</p> required <code>expected_output</code> <code>dict</code> <p>The correct output the model should have produced.</p> required <code>feedback</code> <code>str | None</code> <p>Optional free-text explanation of what went wrong.</p> <code>None</code>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.add_feedback","title":"<code>add_feedback(feedback, level='task', target_id='')</code>","text":"<p>Add user feedback at any level.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>str</code> <p>The feedback text</p> required <code>level</code> <code>str</code> <p>\"task\" (general guidance), \"example\" (specific example),    or \"rule\" (specific rule)</p> <code>'task'</code> <code>target_id</code> <code>str</code> <p>Required for \"example\" and \"rule\" levels \u2014 the id of        the example or rule this feedback applies to</p> <code>''</code>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.add_observation","title":"<code>add_observation(input_data, output_data, metadata=None)</code>","text":"<p>Add a structured LLM observation. Works with any LLM provider.</p> <p>Use this to feed RuleChef data from any source \u2014 Anthropic, Groq, local models, LangChain, etc. You extract the input/output yourself.</p> <p>Unlike add_example(), no task definition is required \u2014 observations can be collected before the task is known.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>Input dict (e.g. {\"text\": \"the query\"}).</p> required <code>output_data</code> <code>dict</code> <p>Output dict (e.g. {\"label\": \"the_class\"}).</p> required <code>metadata</code> <code>dict | None</code> <p>Optional metadata (e.g. {\"model\": \"claude-3\"}).</p> <code>None</code>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.add_raw_observation","title":"<code>add_raw_observation(messages, response, metadata=None)</code>","text":"<p>Add a raw LLM interaction for auto-discovery. Works with any LLM.</p> <p>Use this when you don't know the task schema yet. Pass the raw messages and response text \u2014 RuleChef will analyze these at learn_rules() time to discover the task type, input/output schema, and extract structured training data.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>list[dict]</code> <p>List of message dicts (e.g. [{\"role\": \"user\", \"content\": \"...\"}]).</p> required <code>response</code> <code>str</code> <p>The LLM's response as a plain string.</p> required <code>metadata</code> <code>dict | None</code> <p>Optional metadata (e.g. {\"model\": \"gpt-4o\"}).</p> <code>None</code>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.learn_rules","title":"<code>learn_rules(run_evaluation=None, min_examples=1, max_refinement_iterations=3, sampling_strategy=None, incremental_only=False)</code>","text":"<p>Learn rules from all collected data.</p> <p>This is the core batch learning process. Buffered examples are first committed to the dataset, then rules are synthesized and optionally refined through an evaluate-and-patch loop.</p> <p>Parameters:</p> Name Type Description Default <code>run_evaluation</code> <code>bool | None</code> <p>Whether to run evaluation/refinement loop. None (default) auto-enables if total_data &gt;= 3. True always enables refinement. False disables it (faster, synthesis only).</p> <code>None</code> <code>min_examples</code> <code>int</code> <p>Minimum training items required to proceed.</p> <code>1</code> <code>max_refinement_iterations</code> <code>int</code> <p>Max iterations in refinement loop (1-3).</p> <code>3</code> <code>sampling_strategy</code> <code>str | None</code> <p>Override default sampling strategy for this run. Options: 'balanced', 'recent', 'diversity', 'uncertain', 'varied'.</p> <code>None</code> <code>incremental_only</code> <code>bool</code> <p>If True and rules already exist, only generate patch rules for current failures instead of full re-synthesis.</p> <code>False</code> <p>Returns:</p> Type Description <p>Optional[Tuple[List[Rule], Optional[EvalResult]]]: A tuple of (learned_rules, eval_result) on success. eval_result is None when refinement is disabled. Returns None if not enough data.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.extract","title":"<code>extract(input_data, validate=True)</code>","text":"<p>Extract from input using learned rules.</p> <p>Works for all task types: EXTRACTION, NER, CLASSIFICATION, TRANSFORMATION. Returns empty result if no rules learned, unless llm_fallback=True.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>dict</code> <p>Input data dict matching the task's input_schema.</p> required <code>validate</code> <code>bool</code> <p>If True and output_schema is Pydantic, validate output.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Output dict whose shape depends on the task type: - EXTRACTION: {\"spans\": [{\"text\", \"start\", \"end\", ...}]} - NER: {\"entities\": [{\"text\", \"start\", \"end\", \"type\", ...}]} - CLASSIFICATION: {\"label\": \"class_name\"} - TRANSFORMATION: Dict with keys defined by output_schema.</p> <code>dict</code> <p>Returns an empty structure if no rules matched or none are learned.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.evaluate","title":"<code>evaluate(verbose=True)</code>","text":"<p>Run full entity-level evaluation of current rules against the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, print a formatted evaluation summary to stdout.</p> <code>True</code> <p>Returns:</p> Type Description <code>EvalResult</code> <p>EvalResult with micro/macro P/R/F1, per-class breakdown,</p> <code>EvalResult</code> <p>exact match rate, and failure details.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.get_rule_metrics","title":"<code>get_rule_metrics(verbose=True)</code>","text":"<p>Evaluate each rule individually against the dataset.</p> <p>Useful for identifying dead or harmful rules.</p> <p>Parameters:</p> Name Type Description Default <code>verbose</code> <code>bool</code> <p>If True, print a formatted per-rule metrics table to stdout.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[RuleMetrics]</code> <p>List[RuleMetrics] with per-rule precision/recall/F1, sample matches,</p> <code>list[RuleMetrics]</code> <p>and per-class breakdown.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.delete_rule","title":"<code>delete_rule(rule_id)</code>","text":"<p>Delete a rule by id.</p> <p>Parameters:</p> Name Type Description Default <code>rule_id</code> <code>str</code> <p>The unique identifier of the rule to delete.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the rule was found and deleted, False otherwise.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.get_stats","title":"<code>get_stats()</code>","text":"<p>Get dataset statistics.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dict with keys: 'task' (str), 'dataset' (str), 'corrections' (int),</p> <code>dict</code> <p>'examples' (int), 'feedback' (int), 'rules' (int),</p> <code>dict</code> <p>'description' (str).</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.get_rules_summary","title":"<code>get_rules_summary()</code>","text":"<p>Get formatted summary of learned rules.</p> <p>Returns:</p> Type Description <code>list[dict]</code> <p>List of dicts sorted by priority (descending), each with keys:</p> <code>list[dict]</code> <p>'name' (str), 'description' (str), 'format' (str),</p> <code>list[dict]</code> <p>'priority' (int), 'confidence' (str), 'times_applied' (int),</p> <code>list[dict]</code> <p>'success_rate' (str).</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.discover_task","title":"<code>discover_task()</code>","text":"<p>Discover the task schema from accumulated raw observations.</p> <p>Uses LLM to analyze captured LLM interactions and infer the task type, input/output schema, and text field. Requires at least min_observations_for_discovery raw observations (from start_observing() or add_raw_observation()).</p> <p>Returns:</p> Type Description <code>Task</code> <p>The discovered Task instance (also stored as self.task).</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no raw observations available or using custom extractors.</p> <code>ValueError</code> <p>If not enough observations or LLM returns invalid JSON.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.start_observing","title":"<code>start_observing(openai_client, auto_learn=True, check_interval=60, extract_input=None, extract_output=None, min_observations_for_discovery=5)</code>","text":"<p>Start observing OpenAI-compatible client calls to collect training data.</p> <p>Works in three modes: - Auto mode (task=None): raw capture, schema discovered at learn_rules() time - Mapped mode (task provided): raw capture, LLM maps to schema at learn_rules() - Custom extractor mode (task + extractors): immediate parsing, zero overhead</p> <p>Parameters:</p> Name Type Description Default <code>openai_client</code> <p>OpenAI client (or compatible API).</p> required <code>auto_learn</code> <code>bool</code> <p>If True, automatically triggers learning when coordinator decides.</p> <code>True</code> <code>check_interval</code> <code>int</code> <p>Seconds between coordinator checks (default 60).</p> <code>60</code> <code>extract_input</code> <code>Callable | None</code> <p>Custom function (api_kwargs \u2192 input dict). Optional.</p> <code>None</code> <code>extract_output</code> <code>Callable | None</code> <p>Custom function (response \u2192 output dict). Optional.</p> <code>None</code> <code>min_observations_for_discovery</code> <code>int</code> <p>Min raw observations before auto-discovery can run (default 5). Only relevant when task=None.</p> <code>5</code> <p>Returns:</p> Type Description <p>The same client (monkey-patched in place).</p> Example <p>chef = RuleChef(client=client)  # No task needed wrapped = chef.start_observing(client, auto_learn=False)</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.start_observing--use-wrapped-as-normal-rulechef-captures-calls","title":"Use wrapped as normal \u2014 RuleChef captures calls","text":"<p>response = wrapped.chat.completions.create(...) chef.learn_rules()  # Discovers task + maps + learns</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.start_observing_gliner","title":"<code>start_observing_gliner(gliner_model, method=None, auto_learn=True, check_interval=60)</code>","text":"<p>Observe GLiNER/GLiNER2 predictions to learn rules from them.</p> <p>GLiNER:  auto-patches predict_entities() \u2192 NER observations. GLiNER2: patches the specified method:   - \"extract_entities\" \u2192 NER   - \"classify_text\"    \u2192 CLASSIFICATION   - \"extract_json\"     \u2192 TRANSFORMATION</p> <p>Parameters:</p> Name Type Description Default <code>gliner_model</code> <p>A GLiNER or GLiNER2 model instance.</p> required <code>method</code> <code>str | None</code> <p>Which method to observe (required for GLiNER2 non-NER tasks).</p> <code>None</code> <code>auto_learn</code> <code>bool</code> <p>If True, trigger learning automatically.</p> <code>True</code> <code>check_interval</code> <code>int</code> <p>Seconds between coordinator checks.</p> <code>60</code> <p>Returns:</p> Type Description <p>The same model (monkey-patched in place).</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.stop_observing","title":"<code>stop_observing()</code>","text":"<p>Stop observing LLM calls and background learning.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.stop_observing_gliner","title":"<code>stop_observing_gliner()</code>","text":"<p>Stop observing GLiNER predictions.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.trigger_manual_learning","title":"<code>trigger_manual_learning()</code>","text":"<p>Manually trigger learning from buffered examples.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.get_buffer_stats","title":"<code>get_buffer_stats()</code>","text":"<p>Get statistics about buffered examples and observations.</p>"},{"location":"api/rulechef/#rulechef.engine.RuleChef.generate_llm_examples","title":"<code>generate_llm_examples(num_examples=5, seed=42)</code>","text":"<p>Generate synthetic training examples using LLM.</p> <p>Examples go to buffer and can trigger learning if auto_trigger=True.</p>"},{"location":"api/training-logger/","title":"Training Data Logger","text":"<p>Captures LLM calls as structured training data for model distillation.</p>"},{"location":"api/training-logger/#trainingdatalogger","title":"TrainingDataLogger","text":""},{"location":"api/training-logger/#rulechef.training_logger.TrainingDataLogger","title":"<code>TrainingDataLogger(path, run_metadata=None)</code>","text":"<p>Logs LLM calls as training data for model distillation.</p> <p>Each logged call is a (messages, response) pair with metadata. Written as JSONL, one entry per LLM call.</p> <p>Attributes:</p> Name Type Description <code>path</code> <p>Path to the output JSONL file.</p> <code>stats</code> <code>dict[str, int]</code> <p>Dict of call counts by type.</p> <code>run_id</code> <code>dict[str, int]</code> <p>Identifier for this logging session.</p> <p>Initialize the logger.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the output JSONL file. Created if it doesn't exist. Appends if it already exists (safe for multiple runs).</p> required <code>run_metadata</code> <code>dict[str, Any] | None</code> <p>Optional dict of metadata to attach to every entry in this session (e.g. dataset name, model, configuration).</p> <code>None</code>"},{"location":"api/training-logger/#rulechef.training_logger.TrainingDataLogger.count","title":"<code>count</code>  <code>property</code>","text":"<p>Total number of entries logged.</p>"},{"location":"api/training-logger/#rulechef.training_logger.TrainingDataLogger.log","title":"<code>log(call_type, messages, response, metadata=None)</code>","text":"<p>Log a single LLM call.</p> <p>Parameters:</p> Name Type Description Default <code>call_type</code> <code>str</code> <p>Type of call. One of: - \"rule_synthesis\": Initial rule generation from examples - \"rule_synthesis_per_class\": Per-class rule generation - \"rule_patch\": Patch rules for failures - \"guide_refinement\": Coordinator refinement guidance - \"audit_rules\": Rule pruning/merging audit - \"trigger_decision\": Should-learn decision - \"synthetic_generation\": Synthetic example generation</p> required <code>messages</code> <code>list[dict[str, str]]</code> <p>The messages sent to the LLM (system + user).</p> required <code>response</code> <code>str</code> <p>The raw response text from the LLM.</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Additional context for this specific call. Useful fields: - task_name, task_type: What task this is for - dataset_size, num_classes: Data stats - iteration, max_iterations: Where in the refinement loop - eval_before: Metrics before this call's output is applied - num_rules_in_response: How many rules were parsed - response_valid: Whether the response parsed successfully - target_class: For per-class synthesis - num_failures: For patch calls - guidance: For coordinator calls</p> <code>None</code>"},{"location":"getting-started/concepts/","title":"How RuleChef Works","text":"<p>RuleChef turns labeled examples into fast, deterministic rules. The key idea: an LLM writes the rules, but the rules run without any LLM.</p> <pre><code>flowchart LR\n    A[Your Examples] --&gt;|Learning time| B[LLM writes rules]\n    B --&gt; C[Regex / Code / spaCy rules]\n    C --&gt;|Inference time| D[\"Fast local execution\\n&lt; 1ms, no API calls\"]</code></pre> <p>This page explains the core architecture. For API details, see the Quick Start.</p>"},{"location":"getting-started/concepts/#the-pipeline","title":"The Pipeline","text":"<p>When you call <code>chef.learn_rules()</code>, here's what happens:</p> <pre><code>flowchart TD\n    A[\"1. Buffer\\n(collected examples)\"] --&gt; B[\"2. Commit\\n(buffer \u2192 dataset)\"]\n    B --&gt; C[\"3. Synthesis\\n(LLM generates rules)\"]\n    C --&gt; D[\"4. Evaluation\\n(test rules vs examples)\"]\n    D --&gt;|Failures exist| E[\"5. Refinement\\n(LLM patches rules)\"]\n    E --&gt; D\n    D --&gt;|Good enough| F[\"6. Persist\\n(save rules to disk)\"]</code></pre> <p>Each step in detail:</p>"},{"location":"getting-started/concepts/#1-buffer","title":"1. Buffer","text":"<p>When you call <code>add_example()</code> or <code>add_correction()</code>, data goes into a buffer \u2014 not directly into the dataset. This lets you collect many examples before learning, and gives the coordinator a chance to decide when and how to learn.</p> <pre><code>chef.add_example(input1, output1)   # \u2192 buffer\nchef.add_example(input2, output2)   # \u2192 buffer\nchef.add_correction(input3, wrong, correct)  # \u2192 buffer (high priority)\n\n# Nothing has been learned yet \u2014 examples are waiting in the buffer\n</code></pre>"},{"location":"getting-started/concepts/#2-commit","title":"2. Commit","text":"<p>When <code>learn_rules()</code> is called, buffered examples are committed to the dataset. The dataset is the permanent store \u2014 it persists to disk and accumulates across learning rounds.</p>"},{"location":"getting-started/concepts/#3-synthesis","title":"3. Synthesis","text":"<p>The LLM receives a prompt containing:</p> <ul> <li>Task description \u2014 what you're trying to do</li> <li>Training examples \u2014 input/output pairs from the dataset</li> <li>Data evidence \u2014 regex pattern hints from grex (if enabled)</li> <li>User feedback \u2014 any guidance you've added</li> <li>Format instructions \u2014 how to write regex/code/spaCy rules</li> </ul> <p>The LLM returns a set of rules in JSON format. Each rule has a pattern (regex, code function, or spaCy matcher) and an output template.</p> <p>For multi-class tasks (NER, classification), synthesis can run per-class \u2014 one LLM call per class, each with positive examples (capped to <code>max_samples</code>) and counter-examples from other classes. See Prompt Size Controls for details.</p>"},{"location":"getting-started/concepts/#4-evaluation","title":"4. Evaluation","text":"<p>Rules are tested against the dataset. RuleChef computes:</p> <ul> <li>Per-example: did the rules produce the correct output?</li> <li>Per-class: precision, recall, F1 for each label</li> <li>Overall: micro/macro F1, exact match accuracy</li> </ul> <p>Any examples where rules fail are collected as failures.</p>"},{"location":"getting-started/concepts/#5-refinement","title":"5. Refinement","text":"<p>If failures exist and iterations remain, RuleChef sends the LLM a patch prompt containing:</p> <ul> <li>Current rules (what's already working)</li> <li>Specific failures (what's broken, with expected vs actual output)</li> <li>Coordinator guidance (which classes to focus on)</li> </ul> <p>The LLM generates patch rules targeted at the failures. These are merged into the existing ruleset \u2014 stable rules are preserved.</p>"},{"location":"getting-started/concepts/#6-persist","title":"6. Persist","text":"<p>The final rules are saved to disk as JSON. When you create a new <code>RuleChef</code> with the same <code>dataset_name</code> and <code>storage_path</code>, rules load automatically.</p>"},{"location":"getting-started/concepts/#what-are-rules","title":"What Are Rules?","text":"<p>A rule is a pattern that maps input text to structured output. There are three formats:</p>"},{"location":"getting-started/concepts/#regex-rules","title":"Regex Rules","text":"<p>A regex pattern with an output template. When the pattern matches, the template fills in the output:</p> <pre><code>Pattern:  (?i)\\b(\\d+\\s*mg)\\b\nTemplate: {\"text\": \"$0\", \"type\": \"DOSAGE\", \"start\": \"$start\", \"end\": \"$end\"}\n</code></pre> <p><code>$0</code> is the full match, <code>$1</code>/<code>$2</code> are capture groups, <code>$start</code>/<code>$end</code> are character offsets. The rule engine computes positions \u2014 the LLM never predicts offsets.</p>"},{"location":"getting-started/concepts/#code-rules","title":"Code Rules","text":"<p>A Python function that takes input data and returns structured output:</p> <pre><code>def extract(input_data):\n    import re\n    text = input_data[\"text\"]\n    spans = []\n    for m in re.finditer(r'\\b[A-Z][a-z]+ine\\b', text):\n        spans.append({\"text\": m.group(), \"start\": m.start(), \"end\": m.end(), \"type\": \"DRUG\"})\n    return spans\n</code></pre> <p>Code rules run in a restricted sandbox \u2014 no imports, no file access, no network calls.</p>"},{"location":"getting-started/concepts/#spacy-rules","title":"spaCy Rules","text":"<p>Token or dependency matcher patterns using linguistic attributes (POS tags, lemmas, dependency relations). Requires <code>pip install rulechef[spacy]</code>.</p>"},{"location":"getting-started/concepts/#schemas","title":"Schemas","text":""},{"location":"getting-started/concepts/#task-definition","title":"Task Definition","text":"<p>A <code>Task</code> tells RuleChef what you're trying to do:</p> <pre><code>task = Task(\n    name=\"Medical NER\",\n    description=\"Extract drugs, dosages, and conditions\",\n    input_schema={\"text\": \"str\"},\n    output_schema={\"entities\": \"List[{text: str, start: int, end: int, type: DRUG|DOSAGE|CONDITION}]\"},\n    type=TaskType.NER,\n    text_field=\"text\",\n)\n</code></pre> <ul> <li><code>input_schema</code> \u2014 tells the LLM what input fields exist (so it writes <code>input_data[\"text\"]</code> in code rules)</li> <li><code>output_schema</code> \u2014 tells the LLM what output structure to produce (entity fields, label names)</li> <li><code>type</code> \u2014 determines evaluation logic and prompt templates</li> <li><code>text_field</code> \u2014 which input field regex/spaCy rules match against</li> </ul> <p>Schemas are documentation for the LLM, not executed code. You can also use Pydantic models for type-safe validation.</p>"},{"location":"getting-started/concepts/#coordinators","title":"Coordinators","text":"<p>A coordinator decides when to learn and how to guide refinement:</p> Coordinator How It Decides <code>SimpleCoordinator</code> Threshold-based: learn after N examples, refine after M corrections <code>AgenticCoordinator</code> LLM-guided: analyzes per-class metrics, focuses on weak classes, stops when performance plateaus <p>The agentic coordinator also supports rule pruning \u2014 merging redundant rules and removing noise after learning.</p> <p>See Coordinators for details.</p>"},{"location":"getting-started/concepts/#the-llms-role","title":"The LLM's Role","text":"<p>The LLM is used only during learning \u2014 never during extraction:</p> Operation LLM Used? When <code>add_example()</code> No Just stores in buffer <code>learn_rules()</code> Yes Synthesis + refinement prompts <code>extract()</code> No Runs rules locally <code>evaluate()</code> No Runs rules locally <p>After learning, the rules are self-contained. You can serialize them, ship them to a different machine, and run them without any LLM access.</p>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start \u2014 code examples for all task types</li> <li>Learning &amp; Refinement \u2014 buffer architecture, sampling strategies, incremental patching</li> <li>Coordinators \u2014 simple vs agentic, rule pruning</li> <li>Evaluation &amp; Feedback \u2014 metrics, corrections, custom matchers</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#basic-install","title":"Basic Install","text":"<pre><code>pip install rulechef\n</code></pre> <p>This installs the core library with <code>openai</code> and <code>pydantic</code> as dependencies.</p>"},{"location":"getting-started/installation/#extras","title":"Extras","text":"<p>RuleChef provides optional extras for additional features:</p> <pre><code># Regex pattern suggestions from examples (recommended)\npip install rulechef[grex]\n\n# spaCy token/dependency matcher patterns\npip install rulechef[spacy]\n\n# LLM-powered coordinator for adaptive learning loops\npip install rulechef[agentic]\n\n# Everything\npip install rulechef[all]\n</code></pre>"},{"location":"getting-started/installation/#extra-details","title":"Extra Details","text":"Extra What It Adds When You Need It <code>grex</code> grex regex inference Improves regex rule quality by suggesting patterns from examples <code>spacy</code> spaCy NLP Linguistic patterns using POS, dependency, lemma attributes <code>agentic</code> pydantic-ai LLM-driven coordinator that guides refinement loops <code>all</code> All of the above Full feature set"},{"location":"getting-started/installation/#development-extras","title":"Development Extras","text":"<pre><code># Run tests\npip install rulechef[dev]\n\n# Build documentation\npip install rulechef[docs]\n\n# Run benchmarks\npip install rulechef[benchmark]\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>An OpenAI-compatible API key (OpenAI, Groq, Together, etc.)</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>from rulechef import RuleChef, Task, TaskType, RuleFormat\nprint(\"RuleChef installed successfully\")\n</code></pre>"},{"location":"getting-started/installation/#using-with-other-llm-providers","title":"Using with Other LLM Providers","text":"<p>RuleChef uses the OpenAI client, which works with any OpenAI-compatible API:</p> <pre><code>from openai import OpenAI\n\n# Groq\nclient = OpenAI(\n    api_key=\"your-groq-key\",\n    base_url=\"https://api.groq.com/openai/v1\"\n)\n\n# Together AI\nclient = OpenAI(\n    api_key=\"your-together-key\",\n    base_url=\"https://api.together.xyz/v1\"\n)\n\nchef = RuleChef(task, client, model=\"your-model-name\")\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide shows complete examples for all four task types. If you want to understand the architecture first (buffer, synthesis, rules), see How It Works.</p>"},{"location":"getting-started/quickstart/#setup","title":"Setup","text":"<pre><code>from openai import OpenAI\nfrom rulechef import RuleChef, Task, TaskType\n\nclient = OpenAI()  # Uses OPENAI_API_KEY env var\n</code></pre>"},{"location":"getting-started/quickstart/#extraction","title":"Extraction","text":"<p>Extract untyped text spans from input:</p> <pre><code>task = Task(\n    name=\"Q&amp;A Extraction\",\n    description=\"Extract answer spans from context\",\n    input_schema={\"question\": \"str\", \"context\": \"str\"},\n    output_schema={\"spans\": \"List[Span]\"},\n    type=TaskType.EXTRACTION,\n)\n\nchef = RuleChef(task, client)\n\nchef.add_example(\n    {\"question\": \"When?\", \"context\": \"Built in 1991\"},\n    {\"spans\": [{\"text\": \"1991\", \"start\": 9, \"end\": 13}]}\n)\nchef.add_example(\n    {\"question\": \"When?\", \"context\": \"Released in 2005\"},\n    {\"spans\": [{\"text\": \"2005\", \"start\": 12, \"end\": 16}]}\n)\n\nrules, eval_result = chef.learn_rules()\nprint(f\"Learned {len(rules)} rules\")\n\nresult = chef.extract({\"question\": \"When?\", \"context\": \"Founded in 1997\"})\nprint(result)  # {\"spans\": [{\"text\": \"1997\", ...}]}\n</code></pre>"},{"location":"getting-started/quickstart/#named-entity-recognition-ner","title":"Named Entity Recognition (NER)","text":"<p>Extract typed entities with Pydantic schema validation:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Literal\n\nclass Entity(BaseModel):\n    text: str\n    start: int\n    end: int\n    type: Literal[\"DRUG\", \"DOSAGE\", \"CONDITION\"]\n\nclass NEROutput(BaseModel):\n    entities: List[Entity]\n\ntask = Task(\n    name=\"Medical NER\",\n    description=\"Extract drugs, dosages, and conditions\",\n    input_schema={\"text\": \"str\"},\n    output_schema=NEROutput,\n    type=TaskType.NER,\n)\n\nchef = RuleChef(task, client)\nchef.add_example(\n    {\"text\": \"Take Aspirin 500mg for headache\"},\n    {\"entities\": [\n        {\"text\": \"Aspirin\", \"start\": 5, \"end\": 12, \"type\": \"DRUG\"},\n        {\"text\": \"500mg\", \"start\": 13, \"end\": 18, \"type\": \"DOSAGE\"},\n        {\"text\": \"headache\", \"start\": 23, \"end\": 31, \"type\": \"CONDITION\"},\n    ]}\n)\nchef.learn_rules()\n</code></pre>"},{"location":"getting-started/quickstart/#classification","title":"Classification","text":"<p>Classify text into categories:</p> <pre><code>task = Task(\n    name=\"Intent Classification\",\n    description=\"Classify banking customer queries\",\n    input_schema={\"text\": \"str\"},\n    output_schema={\"label\": \"str\"},\n    type=TaskType.CLASSIFICATION,\n    text_field=\"text\",\n)\n\nchef = RuleChef(task, client)\nchef.add_example({\"text\": \"what is the exchange rate?\"}, {\"label\": \"exchange_rate\"})\nchef.add_example({\"text\": \"I want to know the rates\"}, {\"label\": \"exchange_rate\"})\nchef.add_example({\"text\": \"my card hasn't arrived\"}, {\"label\": \"card_arrival\"})\n\nchef.learn_rules()\nresult = chef.extract({\"text\": \"current exchange rate please\"})\nprint(result)  # {\"label\": \"exchange_rate\"}\n</code></pre>"},{"location":"getting-started/quickstart/#transformation","title":"Transformation","text":"<p>Extract structured fields from text:</p> <pre><code>task = Task(\n    name=\"Invoice Parser\",\n    description=\"Extract company and amount from invoices\",\n    input_schema={\"text\": \"str\"},\n    output_schema={\"company\": \"str\", \"amount\": \"str\"},\n    type=TaskType.TRANSFORMATION,\n)\n\nchef = RuleChef(task, client)\nchef.add_example(\n    {\"text\": \"Invoice from Acme Corp for $1,500.00\"},\n    {\"company\": \"Acme Corp\", \"amount\": \"$1,500.00\"}\n)\nchef.learn_rules()\n\nresult = chef.extract({\"text\": \"Invoice from Globex Inc for $2,300.00\"})\nprint(result)  # {\"company\": \"Globex Inc\", \"amount\": \"$2,300.00\"}\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Task Types \u2014 detailed guide for each type</li> <li>Learning &amp; Refinement \u2014 buffer architecture and incremental patching</li> <li>Evaluation &amp; Feedback \u2014 measure and improve rule quality</li> </ul>"},{"location":"guide/advanced/","title":"Advanced Features","text":""},{"location":"guide/advanced/#observation-mode","title":"Observation Mode","text":"<p>RuleChef can learn from your existing LLM pipeline. Collect observations from any LLM provider \u2014 no task definition needed upfront.</p>"},{"location":"guide/advanced/#structured-observations-add_observation","title":"Structured observations (<code>add_observation</code>)","text":"<p>When you know the input/output shape, pass structured data directly:</p> <pre><code>from rulechef import RuleChef\n\nchef = RuleChef(client=client, model=\"gpt-4o-mini\")  # No task needed\n\n# Works with any LLM \u2014 Anthropic, Groq, local models, etc.\nresponse = anthropic_client.messages.create(\n    model=\"claude-3-haiku-20240307\",\n    messages=[{\"role\": \"user\", \"content\": f\"Classify: {query}\"}],\n)\nchef.add_observation(\n    {\"text\": query},\n    {\"label\": response.content[0].text.strip()},\n)\n\n# After collecting enough observations, learn rules\nchef.learn_rules()\n</code></pre>"},{"location":"guide/advanced/#raw-observations-add_raw_observation","title":"Raw observations (<code>add_raw_observation</code>)","text":"<p>When you don't know the schema, pass raw messages and let RuleChef discover it:</p> <pre><code>chef = RuleChef(client=client, model=\"gpt-4o-mini\")\n\n# Capture the raw interaction \u2014 RuleChef figures out the schema later\nfor query in queries:\n    response = any_llm_call(query)\n    chef.add_raw_observation(\n        messages=[{\"role\": \"user\", \"content\": query}],\n        response=response,\n    )\n\n# Discovers task schema + maps observations + learns rules\nchef.learn_rules()\nprint(chef.task.to_dict())  # See what was discovered\n</code></pre>"},{"location":"guide/advanced/#auto-capture-for-openai-clients-start_observing","title":"Auto-capture for OpenAI clients (<code>start_observing</code>)","text":"<p>For OpenAI-compatible clients, monkey-patch to capture calls automatically:</p> <pre><code>wrapped = chef.start_observing(openai_client, auto_learn=False)\n\n# Use wrapped as normal \u2014 every call is captured\nresponse = wrapped.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": query}],\n)\n\nchef.learn_rules()   # Discovers + maps + learns\nchef.stop_observing()\n</code></pre> <p>When <code>auto_learn=True</code>, learning triggers automatically based on the coordinator's decision. Streaming calls (<code>stream=True</code>) are also observed \u2014 RuleChef wraps the stream to capture content after it completes.</p>"},{"location":"guide/advanced/#gliner-gliner2-observation-start_observing_gliner","title":"GLiNER / GLiNER2 observation (<code>start_observing_gliner</code>)","text":"<p>Observe predictions from GLiNER (NER) or GLiNER2 (NER, classification, structured extraction) models:</p> <pre><code>from gliner import GLiNER\n\nmodel = GLiNER.from_pretrained(\"urchade/gliner_multi-v2.1\")\n\nchef = RuleChef(client=client, model=\"gpt-4o-mini\")\nchef.start_observing_gliner(model, auto_learn=False)\n\n# Use the model as normal \u2014 predictions are captured\nentities = model.predict_entities(\"Apple was founded by Steve Jobs.\", [\"company\", \"person\"])\n\nchef.learn_rules()\nchef.stop_observing_gliner()\n</code></pre> <p>For GLiNER2, specify which method to observe:</p> <pre><code>from gliner2 import GLiNER2\n\nmodel = GLiNER2.from_pretrained(\"fastino/gliner2\")\n\n# NER\nchef.start_observing_gliner(model, method=\"extract_entities\", auto_learn=False)\n\n# Classification\nchef.start_observing_gliner(model, method=\"classify_text\", auto_learn=False)\n\n# Structured extraction\nchef.start_observing_gliner(model, method=\"extract_json\", auto_learn=False)\n</code></pre> <p>No LLM calls are needed for task discovery \u2014 GLiNER output is already structured. The task type, schema, and labels are inferred automatically from the observed predictions.</p>"},{"location":"guide/advanced/#training-data-logger-distillation","title":"Training Data Logger (Distillation)","text":"<p>RuleChef can capture every LLM call made during rule synthesis as structured training data, suitable for fine-tuning a smaller model to replace the LLM. The logger is fully optional \u2014 pass a <code>TrainingDataLogger</code> instance and all calls (synthesis, patching, coordination, auditing) are written to a JSONL file.</p> <pre><code>from rulechef import RuleChef, TrainingDataLogger\n\nlogger = TrainingDataLogger(\n    \"training_data/run_001.jsonl\",\n    run_metadata={\"model\": \"kimi-k2\", \"dataset\": \"banking77\"},\n)\nchef = RuleChef(task, client, training_logger=logger)\n\nchef.add_example(...)\nchef.learn_rules()\n\nprint(logger.stats)   # {\"rule_synthesis\": 5, \"rule_patch\": 3, \"guide_refinement\": 10, ...}\nprint(logger.count)    # 18 total entries\n</code></pre>"},{"location":"guide/advanced/#output-format","title":"Output format","text":"<p>Each line in the JSONL file is a self-contained training example:</p> <pre><code>{\n  \"messages\": [\n    {\"role\": \"user\", \"content\": \"...prompt...\"},\n    {\"role\": \"assistant\", \"content\": \"...response...\"}\n  ],\n  \"call_type\": \"rule_synthesis\",\n  \"metadata\": {\n    \"model\": \"kimi-k2\",\n    \"dataset\": \"banking77\",\n    \"task_name\": \"Intent Classification\",\n    \"dataset_size\": 25,\n    \"num_rules_in_response\": 8,\n    \"response_valid\": true\n  },\n  \"timestamp\": \"2026-02-19T14:30:00+00:00\"\n}\n</code></pre> <p>For fine-tuning, use only the <code>messages</code> field. The <code>metadata</code> and <code>call_type</code> are for filtering \u2014 e.g. keep only entries where <code>response_valid</code> is true, or only runs where the final F1 exceeded a threshold.</p>"},{"location":"guide/advanced/#call-types","title":"Call types","text":"Call type Source Description <code>rule_synthesis</code> Learner Bulk rule generation from examples <code>rule_synthesis_per_class</code> Learner Per-class rule generation <code>rule_patch</code> Learner Patch rules targeted at failures <code>synthetic_generation</code> Learner Synthetic example generation <code>guide_refinement</code> Coordinator Per-iteration refinement guidance <code>audit_rules</code> Coordinator Rule pruning/merging audit <code>trigger_decision</code> Coordinator Should-learn decision"},{"location":"guide/advanced/#generating-training-data-at-scale","title":"Generating training data at scale","text":"<p>To generate a diverse training corpus, run RuleChef across multiple datasets with varied configurations:</p> <pre><code>import itertools\nfrom rulechef import RuleChef, TrainingDataLogger, AgenticCoordinator\n\ndatasets = [\"banking77\", \"clinc150\", \"snips\", ...]\nshots = [3, 5, 10]\n\nfor ds_name, n_shots in itertools.product(datasets, shots):\n    logger = TrainingDataLogger(\n        f\"training_data/{ds_name}_{n_shots}shot.jsonl\",\n        run_metadata={\"dataset\": ds_name, \"shots\": n_shots},\n    )\n    coordinator = AgenticCoordinator(client, training_logger=logger)\n    chef = RuleChef(task, client, coordinator=coordinator, training_logger=logger)\n    # ... add examples, learn rules ...\n</code></pre> <p>Tip</p> <p>The logger appends to the file, so multiple runs can safely write to the same path. Each entry carries its own <code>run_metadata</code> and timestamp.</p>"},{"location":"guide/advanced/#pydantic-output-schemas","title":"Pydantic Output Schemas","text":"<p>Use Pydantic models for type-safe, validated outputs:</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Literal\n\nclass Entity(BaseModel):\n    text: str\n    start: int\n    end: int\n    type: Literal[\"PERSON\", \"ORG\", \"LOCATION\"]\n\nclass Output(BaseModel):\n    entities: List[Entity]\n\ntask = Task(\n    name=\"NER\",\n    description=\"Extract entities\",\n    input_schema={\"text\": \"str\"},\n    output_schema=Output,\n    type=TaskType.NER,\n)\n</code></pre> <p>RuleChef automatically:</p> <ul> <li>Discovers valid labels from <code>Literal</code> type annotations</li> <li>Validates rule outputs against the model at runtime</li> <li>Generates readable schema fragments for synthesis prompts</li> </ul>"},{"location":"guide/advanced/#output-templates","title":"Output Templates","text":"<p>Rules can emit structured JSON using template variables:</p>"},{"location":"guide/advanced/#regex-templates","title":"Regex Templates","text":"Variable Meaning <code>$0</code> Full match text <code>$1</code>, <code>$2</code>, ... Capture groups <code>$start</code>, <code>$end</code> Match offsets <pre><code>{\n  \"output_template\": {\n    \"text\": \"$1\",\n    \"type\": \"DRUG\",\n    \"start\": \"$start\",\n    \"end\": \"$end\"\n  }\n}\n</code></pre>"},{"location":"guide/advanced/#spacy-templates","title":"spaCy Templates","text":"Variable Meaning <code>$1.text</code>, <code>$2.text</code> Token text <code>$1.start</code>, <code>$1.end</code> Token character offsets"},{"location":"guide/advanced/#spacy-patterns","title":"spaCy Patterns","text":""},{"location":"guide/advanced/#token-matcher","title":"Token Matcher","text":"<p>Use token attributes for linguistic patterns:</p> <pre><code>[\n  {\"POS\": \"PROPN\", \"OP\": \"+\"},\n  {\"POS\": \"NOUN\"}\n]\n</code></pre> <p>Available attributes: <code>TEXT</code>, <code>LOWER</code>, <code>POS</code>, <code>TAG</code>, <code>DEP</code>, <code>LEMMA</code>, <code>SHAPE</code>, <code>IS_ALPHA</code>, <code>IS_DIGIT</code>, <code>OP</code>.</p>"},{"location":"guide/advanced/#dependency-matcher","title":"Dependency Matcher","text":"<p>Match syntactic structure:</p> <pre><code>[\n  {\"RIGHT_ID\": \"verb\", \"RIGHT_ATTRS\": {\"POS\": \"VERB\"}},\n  {\"LEFT_ID\": \"verb\", \"REL_OP\": \"&gt;\", \"RIGHT_ID\": \"subj\", \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"}}\n]\n</code></pre> <p>spaCy NER</p> <p>By default, <code>use_spacy_ner=False</code> \u2014 spaCy's NER pipe is disabled and patterns relying on <code>ENT_TYPE</code> are rejected. Set <code>use_spacy_ner=True</code> to enable.</p>"},{"location":"guide/advanced/#llm-fallback","title":"LLM Fallback","text":"<p>When rules produce no results, optionally fall back to direct LLM extraction:</p> <pre><code>chef = RuleChef(task, client, llm_fallback=True)\n\nresult = chef.extract({\"text\": \"unusual input\"})\n# If no rule matches \u2192 calls LLM directly\n</code></pre>"},{"location":"guide/advanced/#using-grex-for-regex-suggestions","title":"Using grex for Regex Suggestions","text":"<p>grex is a library that infers regex patterns from example strings. RuleChef uses it to give the LLM concrete pattern suggestions during rule synthesis.</p>"},{"location":"guide/advanced/#install","title":"Install","text":"<p>grex is an optional dependency:</p> <pre><code>pip install rulechef[grex]\n</code></pre> <p>It's enabled by default when installed. To disable:</p> <pre><code>chef = RuleChef(task, client, use_grex=False)\n</code></pre>"},{"location":"guide/advanced/#what-grex-does","title":"What grex does","text":"<p>You give grex a list of strings, it gives you a regex that matches all of them:</p> <pre><code>from grex import RegExpBuilder\n\ndates = [\"2024-01-15\", \"2024-02-28\", \"2023-12-01\"]\n\n# Exact: alternation of all inputs\nRegExpBuilder.from_test_cases(dates).without_anchors().build()\n# \u2192 '(2023\\-12\\-01|2024\\-01\\-15|2024\\-02\\-28)'\n\n# Generalized: replaces digits/repetitions with character classes\nRegExpBuilder.from_test_cases(dates).without_anchors() \\\n    .with_conversion_of_digits().with_conversion_of_repetitions().build()\n# \u2192 '\\d{4}\\-\\d{2}\\-\\d{2}'\n</code></pre> <p>The generalized pattern <code>\\d{4}\\-\\d{2}\\-\\d{2}</code> matches any date in that format, not just the three examples. This is what makes grex valuable \u2014 it finds structure.</p>"},{"location":"guide/advanced/#how-rulechef-uses-it","title":"How RuleChef uses it","text":"<p>During rule synthesis, RuleChef builds a \"data evidence\" section in the prompt. Without grex, the LLM only sees raw example strings:</p> <pre><code>DATA EVIDENCE FROM TRAINING:\n- exchange_rate (3 examples): \"what is the exchange rate for USD to EUR?\", \"how much is a dollar in euros?\", \"I want to know the current rates\"\n- card_arrival (3 examples): \"my new card still hasn't arrived\", \"when will my new card be delivered?\", \"my card hasn't come in the mail yet\"\n</code></pre> <p>With grex enabled, each group gets regex pattern suggestions appended:</p> <pre><code>DATA EVIDENCE FROM TRAINING:\n- exchange_rate (3 examples): \"what is the exchange rate for USD to EUR?\", ...\n  Exact pattern: (I want to know the current rates|how much is a dollar in euros\\?|what is the exchange rate for USD to EUR\\?)\n- card_arrival (3 examples): \"my new card still hasn't arrived\", ...\n  Exact pattern: (my card hasn't come in the mail yet|my new card still hasn't arrived|when will my new card be delivered\\?)\n</code></pre> <p>grex generates two types of patterns:</p> <ul> <li>Exact pattern \u2014 alternation of all seen strings (always included)</li> <li>Structural pattern \u2014 generalized version with digit/repetition conversion (included when it's meaningfully shorter than the exact pattern)</li> </ul> <p>For NER and transformation tasks with structured values (dates, IDs, codes), the structural pattern is especially valuable:</p> <pre><code>- DATE (5 unique): \"2024-01-15\", \"2024-02-28\", \"2023-12-01\", ...\n  Exact pattern: (2023\\-12\\-01|2024\\-01\\-15|2024\\-02\\-28|...)\n  Structural pattern: \\d{4}\\-\\d{2}\\-\\d{2}\n</code></pre> <p>The structural pattern <code>\\d{4}\\-\\d{2}\\-\\d{2}</code> tells the LLM to write a general date regex rather than hardcoding the specific dates.</p>"},{"location":"guide/advanced/#when-grex-helps-most","title":"When grex helps most","text":"<ul> <li>Structured extraction \u2014 dates, phone numbers, IDs, codes, amounts</li> <li>NER \u2014 entity strings with consistent patterns (drug names, gene symbols)</li> <li>Classification with keyword clusters \u2014 groups of similar input phrases</li> </ul>"},{"location":"guide/advanced/#when-grex-doesnt-help","title":"When grex doesn't help","text":"<ul> <li>Very long input strings (&gt;80 chars are skipped)</li> <li>Fewer than 2 unique strings per group</li> <li>Highly diverse strings with no shared structure (exact pattern becomes a giant alternation that the LLM ignores)</li> </ul>"},{"location":"guide/advanced/#debugging","title":"Debugging","text":"<p>Set the environment variable to see when grex is used:</p> <pre><code>RULECHEF_GREX_LOG=1 python your_script.py\n</code></pre> <p>This prints lines like <code>[rulechef][grex] used CLASSIFICATION:exchange_rate</code> whenever a pattern is generated.</p>"},{"location":"guide/advanced/#code-rule-security","title":"Code Rule Security","text":"<p>Code rules (<code>RuleFormat.CODE</code>) are executed via Python's <code>exec()</code> in a restricted namespace. The default <code>__builtins__</code> are replaced with a curated safe subset, so code rules cannot import modules, access the filesystem, or execute arbitrary code.</p> <p>This means code rules can use:</p> <ul> <li><code>re</code> \u2014 the standard library regex module</li> <li><code>Span</code> \u2014 RuleChef's span dataclass for returning results</li> <li>Safe builtins \u2014 <code>len</code>, <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, <code>list</code>, <code>dict</code>, <code>set</code>, <code>tuple</code>, <code>range</code>, <code>enumerate</code>, <code>zip</code>, <code>map</code>, <code>filter</code>, <code>sorted</code>, <code>reversed</code>, <code>min</code>, <code>max</code>, <code>sum</code>, <code>any</code>, <code>all</code>, <code>abs</code>, <code>round</code>, <code>isinstance</code>, <code>type</code>, <code>print</code></li> <li>Basic Python syntax (loops, conditionals, string methods, list comprehensions)</li> </ul> <p>Code rules cannot:</p> <ul> <li>Import modules (<code>import os</code>, <code>__import__('subprocess')</code>)</li> <li>Access files or environment variables</li> <li>Make network calls</li> <li>Call <code>open()</code>, <code>eval()</code>, <code>exec()</code>, <code>getattr()</code>, or <code>compile()</code></li> </ul> <p>If you need capabilities beyond this, use regex or spaCy rules instead.</p>"},{"location":"guide/advanced/#cli","title":"CLI","text":"<p>Interactive CLI for quick experimentation:</p> <pre><code>export OPENAI_API_KEY=your_key\nrulechef\n</code></pre>"},{"location":"guide/app/","title":"Web App","text":"<p>A web UI for RuleChef \u2014 upload data, learn rules via LLM, see highlighted entities, and correct mistakes to improve rules.</p>"},{"location":"guide/app/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li>Node.js 20+</li> <li>An OpenAI-compatible API key (Groq, OpenAI, Together, etc.)</li> </ul>"},{"location":"guide/app/#setup","title":"Setup","text":""},{"location":"guide/app/#backend","title":"Backend","text":"<pre><code>pip install -e \".[app]\"\nexport OPENAI_API_KEY=gsk_...   # your API key\nuvicorn api.main:app --reload --port 8000\n</code></pre> <p>The backend defaults to Kimi K2 (<code>moonshotai/kimi-k2-instruct-0905</code>) via the Groq API. To use a different provider:</p> <pre><code>export OPENAI_BASE_URL=https://api.openai.com/v1/   # or any OpenAI-compatible API\nexport OPENAI_MODEL=gpt-4o-mini\n</code></pre> <p>If you're developing the <code>rulechef</code> library locally alongside the app:</p> <pre><code>pip install -e /path/to/rulechef\n</code></pre>"},{"location":"guide/app/#frontend","title":"Frontend","text":"<pre><code>cd frontend\nnpm install\nnpm run dev\n</code></pre> <p>Open http://localhost:5173. The dev server proxies <code>/api</code> requests to the backend on port 8000.</p>"},{"location":"guide/app/#usage","title":"Usage","text":"<ol> <li>Data page \u2014 Set entity labels, then add training examples by pasting text and highlighting spans with the annotation tool. Or bulk upload CSV/JSON.</li> <li>Learn page \u2014 Click \"Learn Rules\" to trigger LLM-powered rule learning. Watch metrics and inspect learned rules.</li> <li>Extract page \u2014 Enter text, run extraction, review highlighted entities. Select text to add entities, click highlights to change or remove them. Submit corrections to feed back into learning.</li> </ol> <p>A default NER project is created automatically \u2014 no configuration step needed.</p>"},{"location":"guide/app/#multi-user-sessions","title":"Multi-user sessions","text":"<p>Each browser tab gets its own isolated session. Multiple people can use the app simultaneously without interfering with each other. Refreshing the page starts a fresh session. Sessions are automatically cleaned up after 1 hour of inactivity.</p>"},{"location":"guide/app/#production-build","title":"Production Build","text":"<p>Build the frontend and serve everything from FastAPI:</p> <pre><code>cd frontend &amp;&amp; npm run build &amp;&amp; cd ..\nuvicorn api.main:app --host 0.0.0.0 --port 8080\n</code></pre>"},{"location":"guide/app/#project-structure","title":"Project Structure","text":"<pre><code>api/                    FastAPI backend\n  main.py               App entry, CORS, static files\n  config.py             Settings from env vars\n  state.py              Per-session RuleChef instances\n  schemas.py            Pydantic request/response models\n  tasks.py              Background learning runner\n  routes/\n    project.py          Configure task, get status, default project\n    data.py             Upload, add examples/corrections\n    learning.py         Trigger learning, poll status\n    extraction.py       Run extraction\n    rules.py            List/delete rules\nfrontend/               Vite + React + TypeScript + shadcn/ui\n  src/\n    api/                Fetch wrapper + React Query hooks\n    pages/              DataPage, LearnPage, ExtractPage\n    components/\n      layout/           AppShell with top nav\n      data/             FileUpload, ExampleTable\n      learning/         LearnButton, RulesTable, MetricsCard\n      extraction/       AnnotatedText, EntityLegend, LabelPicker, CorrectionToolbar\n</code></pre>"},{"location":"guide/app/#api-endpoints","title":"API Endpoints","text":"Method Endpoint Description POST <code>/api/project/configure</code> Create task + RuleChef instance GET <code>/api/project/status</code> Current state + stats POST <code>/api/data/upload</code> Upload CSV/JSON training data POST <code>/api/data/example</code> Add single example POST <code>/api/data/correction</code> Submit user correction GET <code>/api/data/examples</code> List examples + corrections POST <code>/api/learn</code> Trigger learning (background) GET <code>/api/learn/status</code> Poll learning progress POST <code>/api/extract</code> Run extraction on input POST <code>/api/extract/batch</code> Batch extraction GET <code>/api/rules</code> List learned rules DELETE <code>/api/rules/{id}</code> Delete a rule"},{"location":"guide/app/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>OPENAI_API_KEY</code> \u2014 API key (Groq, OpenAI, or any compatible provider) <code>OPENAI_BASE_URL</code> <code>https://api.groq.com/openai/v1/</code> LLM API base URL <code>OPENAI_MODEL</code> <code>moonshotai/kimi-k2-instruct-0905</code> Model name <code>RULECHEF_STORAGE_PATH</code> <code>./rulechef_data</code> Directory for persisted datasets <code>SESSION_TTL_SECONDS</code> <code>3600</code> Session expiry (seconds of inactivity) <code>SESSION_SECRET</code> <code>dev-insecure-...</code> Secret for signing session cookies (set in production!)"},{"location":"guide/coordinators/","title":"Coordinators","text":"<p>Coordinators decide when to learn and how to approach refinement. They sit between the buffer and the learner.</p> <pre><code>flowchart LR\n    A[Buffer] --&gt; B[Coordinator]\n    B --&gt;|Learn Now| C[learn_rules]\n    B --&gt;|Wait| A</code></pre>"},{"location":"guide/coordinators/#simplecoordinator","title":"SimpleCoordinator","text":"<p>The default coordinator uses threshold-based heuristics:</p> <pre><code>from rulechef import SimpleCoordinator\n\ncoordinator = SimpleCoordinator(\n    trigger_threshold=10,    # Min new examples before first learn\n    correction_threshold=5,  # Min corrections before refinement\n)\n\nchef = RuleChef(task, client, coordinator=coordinator)\n</code></pre>"},{"location":"guide/coordinators/#decision-logic","title":"Decision Logic","text":"<p>First learn (no rules yet):</p> <ul> <li>Triggers when <code>new_examples &gt;= trigger_threshold</code></li> </ul> <p>Subsequent learns (rules exist):</p> <ul> <li>If <code>new_corrections &gt;= correction_threshold</code> \u2192 <code>corrections_first</code> strategy</li> <li>Elif <code>new_examples &gt;= trigger_threshold</code> \u2192 <code>diversity</code> strategy</li> <li>Otherwise \u2192 wait</li> </ul>"},{"location":"guide/coordinators/#auto-trigger","title":"Auto-Trigger","text":"<p>With <code>auto_trigger=True</code>, the coordinator is checked after every <code>add_example()</code> and <code>add_correction()</code>:</p> <pre><code>chef = RuleChef(task, client,\n    coordinator=coordinator,\n    auto_trigger=True,\n)\n\n# Learning happens automatically when coordinator decides it's time\nchef.add_example(input1, output1)\nchef.add_example(input2, output2)\n# ... after enough examples, learn_rules() fires automatically\n</code></pre>"},{"location":"guide/coordinators/#agenticcoordinator","title":"AgenticCoordinator","text":"<p>The <code>AgenticCoordinator</code> uses LLM calls to guide refinement, focusing on weak classes:</p> <pre><code>from rulechef import AgenticCoordinator\n\ncoordinator = AgenticCoordinator(client, model=\"gpt-4o-mini\")\nchef = RuleChef(task, client, coordinator=coordinator)\n\nchef.learn_rules(max_refinement_iterations=10)\n</code></pre> <p>Each iteration, the agentic coordinator:</p> <ol> <li>Analyzes per-class metrics</li> <li>Identifies weak classes that need more rules</li> <li>Generates targeted guidance for the synthesis prompt</li> <li>Decides when to stop (performance plateau or good enough)</li> </ol> <p>Extra required</p> <p>The agentic coordinator requires <code>pip install rulechef[agentic]</code>.</p>"},{"location":"guide/coordinators/#rule-pruning","title":"Rule Pruning","text":"<p>Over multiple refinement iterations, rulesets can accumulate redundant rules. Enable <code>prune_after_learn</code> to audit and consolidate rules after learning:</p> <pre><code>coordinator = AgenticCoordinator(\n    client,\n    model=\"gpt-4o-mini\",\n    prune_after_learn=True,\n)\nchef = RuleChef(task, client, coordinator=coordinator)\nchef.learn_rules()\n</code></pre> <p>The audit prefers merging over removing:</p> <ul> <li>Merge: Two regex rules with similar patterns targeting the same label get combined into one (e.g. <code>(?:bad|awful)</code> + <code>(?:terrible|worst)</code> \u2192 <code>(?:bad|awful|terrible|worst)</code>)</li> <li>Remove: Only rules that are pure noise (precision=0, all matches are false positives)</li> </ul> <p>A safety net re-evaluates after pruning and reverts all changes if F1 drops by more than 1%. Rules with low scores are kept -- they may cover rare edge cases not in the training set.</p> <p>In the CLI: <code>learn --agentic --prune</code>.</p>"},{"location":"guide/coordinators/#custom-coordinators","title":"Custom Coordinators","text":"<p>Implement the <code>CoordinatorProtocol</code>:</p> <pre><code>from rulechef.coordinator import CoordinatorProtocol, CoordinationDecision, AuditResult\n\nclass MyCoordinator(CoordinatorProtocol):\n    def should_trigger_learning(self, buffer, current_rules):\n        \"\"\"Decide whether to trigger learning.\"\"\"\n        stats = buffer.get_stats()\n        return CoordinationDecision(\n            should_learn=stats[\"new_examples\"] &gt;= 5,\n            strategy=\"balanced\",\n            reasoning=\"Custom logic\",\n            max_iterations=3,\n        )\n\n    def analyze_buffer(self, buffer):\n        \"\"\"Return analysis dict for inspection.\"\"\"\n        return buffer.get_stats()\n\n    def guide_refinement(self, eval_result, iteration, max_iterations):\n        \"\"\"Return (guidance_text, should_continue).\"\"\"\n        if eval_result and eval_result.micro_f1 &gt; 0.9:\n            return \"\", False  # Stop \u2014 good enough\n        return \"Focus on recall\", True\n\n    def on_learning_complete(self, old_rules, new_rules, metrics):\n        \"\"\"Called after learning finishes.\"\"\"\n        pass\n\n    def audit_rules(self, rules, rule_metrics):\n        \"\"\"Return AuditResult with merge/remove actions. Default: no-op.\"\"\"\n        return AuditResult()\n</code></pre>"},{"location":"guide/coordinators/#coordinationdecision-fields","title":"CoordinationDecision Fields","text":"Field Type Description <code>should_learn</code> <code>bool</code> Whether to trigger learning <code>strategy</code> <code>str</code> Sampling strategy (<code>balanced</code>, <code>diversity</code>, <code>corrections_first</code>) <code>reasoning</code> <code>str</code> Human-readable explanation <code>max_iterations</code> <code>int</code> Max refinement iterations (default: 3) <code>metadata</code> <code>dict</code> Arbitrary metadata (default: <code>{}</code>)"},{"location":"guide/evaluation/","title":"Evaluation &amp; Feedback","text":"<p>RuleChef includes built-in evaluation with entity-level precision, recall, and F1 metrics.</p>"},{"location":"guide/evaluation/#dataset-evaluation","title":"Dataset Evaluation","text":"<pre><code>eval_result = chef.evaluate()\n</code></pre> <p>This runs all rules against the dataset and computes:</p> <ul> <li>Exact match accuracy \u2014 percentage of inputs where output matches exactly</li> <li>Micro precision/recall/F1 \u2014 aggregated across all predictions</li> <li>Macro F1 \u2014 averaged across classes (for multi-class tasks)</li> <li>Per-class breakdown \u2014 precision, recall, F1 per label</li> </ul>"},{"location":"guide/evaluation/#per-rule-evaluation","title":"Per-Rule Evaluation","text":"<p>Find which rules are helping and which are hurting:</p> <pre><code>metrics = chef.get_rule_metrics()\n</code></pre> <p>Each rule gets individual metrics:</p> <ul> <li>TP / FP / FN \u2014 true positives, false positives, false negatives</li> <li>Sample matches \u2014 which examples each rule matched</li> <li>Dead rules \u2014 rules that never fire on any example</li> </ul> <pre><code># Delete a rule that's causing false positives\nchef.delete_rule(\"rule_id\")\n</code></pre>"},{"location":"guide/evaluation/#corrections","title":"Corrections","text":"<p>Corrections are the highest-value training signal. They show exactly where current rules fail:</p> <pre><code>result = chef.extract({\"text\": \"some input\"})\n\n# Result was wrong \u2014 correct it\nchef.add_correction(\n    {\"text\": \"some input\"},\n    model_output=result,\n    expected_output={\"label\": \"correct_label\"},\n    feedback=\"The rule matched too broadly\"\n)\n\nchef.learn_rules()  # Re-learns with corrections prioritized\n</code></pre>"},{"location":"guide/evaluation/#feedback","title":"Feedback","text":"<p>Feedback provides guidance at different levels:</p>"},{"location":"guide/evaluation/#task-level-feedback","title":"Task-Level Feedback","text":"<p>General guidance for the entire task:</p> <pre><code>chef.add_feedback(\"Drug names always follow 'take' or 'prescribe'\")\nchef.add_feedback(\"Ignore mentions in parentheses\")\n</code></pre>"},{"location":"guide/evaluation/#rule-level-feedback","title":"Rule-Level Feedback","text":"<p>Guidance targeted at a specific rule:</p> <pre><code>chef.add_feedback(\n    \"This rule is too broad \u2014 it matches common words\",\n    level=\"rule\",\n    target_id=\"rule_123\"\n)\n</code></pre> <p>Feedback is included in synthesis prompts during the next <code>learn_rules()</code> call.</p>"},{"location":"guide/evaluation/#matching-modes","title":"Matching Modes","text":""},{"location":"guide/evaluation/#extraction","title":"Extraction","text":"<pre><code>task = Task(\n    ...,\n    type=TaskType.EXTRACTION,\n    matching_mode=\"text\",   # Compare span text only (default)\n)\n\ntask = Task(\n    ...,\n    type=TaskType.EXTRACTION,\n    matching_mode=\"exact\",  # Compare text + start/end offsets\n)\n</code></pre>"},{"location":"guide/evaluation/#ner","title":"NER","text":"<p>Entity matching checks both text and type. Entities match if they have the same text and entity type.</p>"},{"location":"guide/evaluation/#classification","title":"Classification","text":"<p>Label matching is case-insensitive and strips whitespace.</p>"},{"location":"guide/evaluation/#transformation","title":"Transformation","text":"<p>Dict matching compares values recursively. Array elements are matched order-independently.</p>"},{"location":"guide/evaluation/#custom-matchers","title":"Custom Matchers","text":"<p>Override the default matching logic:</p> <pre><code>def my_matcher(expected, actual):\n    # Custom comparison logic\n    return expected[\"label\"].lower() == actual[\"label\"].lower()\n\ntask = Task(\n    ...,\n    output_matcher=my_matcher,\n)\n</code></pre>"},{"location":"guide/evaluation/#stats","title":"Stats","text":"<p>Get a summary of the current state:</p> <pre><code>stats = chef.get_stats()\n# {\n#   \"dataset_name\": \"default\",\n#   \"total_examples\": 25,\n#   \"total_corrections\": 3,\n#   \"total_rules\": 12,\n#   \"buffer_stats\": {...},\n# }\n\nsummary = chef.get_rules_summary()\n# Human-readable summary of all rules\n</code></pre>"},{"location":"guide/learning/","title":"Learning &amp; Refinement","text":"<p>RuleChef uses a buffer-first architecture: examples are collected, then committed to a dataset and used for rule synthesis during <code>learn_rules()</code>.</p>"},{"location":"guide/learning/#buffer-first-architecture","title":"Buffer-First Architecture","text":"<pre><code>flowchart TD\n    A[add_example / add_correction] --&gt; B[Buffer]\n    B --&gt; C[learn_rules]\n    C --&gt; D[Buffer \u2192 Dataset]\n    D --&gt; E[Rule Synthesis]\n    E --&gt; F[Evaluation &amp; Refinement]\n    F --&gt; G[Persisted Rules]</code></pre> <pre><code>chef.add_example(input1, output1)   # Goes to buffer\nchef.add_example(input2, output2)   # Goes to buffer\nchef.add_correction(input3, wrong, correct)  # High-priority signal\n\nchef.learn_rules()  # Buffer \u2192 Dataset \u2192 Synthesis \u2192 Refinement\n</code></pre>"},{"location":"guide/learning/#learn_rules","title":"learn_rules()","text":"<p>The main learning method orchestrates the full pipeline:</p> <pre><code>rules, eval_result = chef.learn_rules(\n    max_refinement_iterations=3,  # Evaluation-refinement cycles\n    incremental_only=False,       # If True, only patch (don't re-synthesize)\n)\n</code></pre> <p>What happens during <code>learn_rules()</code>:</p> <ol> <li>Buffer examples are committed to the dataset</li> <li>Rules are synthesized using the LLM</li> <li>Rules are evaluated against the dataset</li> <li>Failed examples drive refinement iterations</li> <li>Rules are persisted to disk</li> </ol> <p>Returns: A tuple of <code>(List[Rule], Optional[EvalResult])</code>.</p>"},{"location":"guide/learning/#synthesis-strategies","title":"Synthesis Strategies","text":"<p>For multi-class tasks (NER, classification), RuleChef can synthesize rules per-class for better coverage:</p> <pre><code># Auto-detect: per-class if &gt;1 class, bulk otherwise (default)\nchef = RuleChef(task, client, synthesis_strategy=\"auto\")\n\n# Force per-class synthesis\nchef = RuleChef(task, client, synthesis_strategy=\"per_class\")\n\n# Force single-prompt bulk synthesis\nchef = RuleChef(task, client, synthesis_strategy=\"bulk\")\n</code></pre> <p>Per-class synthesis generates rules for each label separately \u2014 one LLM call per class. Each call receives:</p> <ul> <li>Positive examples for that class (capped to <code>max_samples</code>, sampled using <code>sampling_strategy</code>)</li> <li>Counter-examples from other classes (capped to <code>max_counter_examples</code>) to prevent false positives</li> <li>Up to <code>max_rules_per_class</code> rules are generated per call</li> </ul> <p>For a 5-class classification task with default settings, this means 5 LLM calls, each producing up to 5 rules, with up to 50 positive examples and 10 counter-examples per prompt.</p>"},{"location":"guide/learning/#prompt-size-controls","title":"Prompt Size Controls","text":"<p>These parameters control how much data goes into each LLM prompt:</p> <pre><code>chef = RuleChef(task, client,\n    max_samples=50,             # Max examples per prompt (per-class positives + patch failures)\n    max_rules_per_class=5,      # Max rules generated per class in per-class synthesis\n    max_counter_examples=10,    # Max negative examples per class prompt\n    max_rules=10,               # Max rules per bulk synthesis call\n)\n</code></pre> Parameter Default Applies to <code>max_samples</code> 50 Per-class positive examples and patch failure sampling <code>max_rules_per_class</code> 5 Per-class synthesis (rules generated per class) <code>max_counter_examples</code> 10 Per-class synthesis (negative examples per prompt) <code>max_rules</code> 10 Bulk synthesis (total rules per call) <p>When a class has more examples than <code>max_samples</code>, examples are sampled using the configured <code>sampling_strategy</code>.</p>"},{"location":"guide/learning/#sampling-strategies","title":"Sampling Strategies","text":"<p>Control how training data is selected when there are more examples than <code>max_samples</code>:</p> <pre><code>chef = RuleChef(task, client, sampling_strategy=\"balanced\")\n</code></pre> Strategy Behavior <code>balanced</code> Takes examples in order (default) <code>recent</code> Favor recently added examples <code>diversity</code> Evenly space picks across the dataset <code>uncertain</code> Low-confidence examples first <code>varied</code> Mix of recent, diverse, and uncertain <code>corrections_first</code> Corrections first, then recent examples <p>All strategies always include corrections first \u2014 they're the highest-value signal.</p>"},{"location":"guide/learning/#incremental-patching","title":"Incremental Patching","text":"<p>After the initial learn, you can patch rules for specific failures without re-synthesizing everything:</p> <pre><code># Initial synthesis\nchef.learn_rules()\n\n# Add corrections for failures\nchef.add_correction(\n    {\"text\": \"some input\"},\n    model_output={\"label\": \"wrong\"},\n    expected_output={\"label\": \"correct\"},\n)\n\n# Patch existing rules (don't re-synthesize)\nchef.learn_rules(incremental_only=True)\n</code></pre> <p>Incremental patching:</p> <ul> <li>Generates targeted rules for known failures</li> <li>Merges new rules into the existing ruleset</li> <li>Prunes weak rules that don't contribute</li> <li>Preserves stable rules that are working</li> </ul>"},{"location":"guide/learning/#persistence","title":"Persistence","text":"<p>Rules and datasets are automatically saved to disk:</p> <pre><code>chef = RuleChef(task, client,\n    dataset_name=\"my_project\",       # Filename for the dataset\n    storage_path=\"./rulechef_data\",  # Directory for JSON files\n)\n</code></pre> <p>Files saved:</p> <ul> <li><code>{storage_path}/{dataset_name}.json</code> \u2014 dataset with examples, corrections, rules</li> </ul> <p>Loading happens automatically when a matching file exists at the storage path.</p>"},{"location":"guide/task-types/","title":"Task Types","text":"<p>RuleChef supports four task types, each with a canonical output format.</p>"},{"location":"guide/task-types/#overview","title":"Overview","text":"Type Output Key Output Format Use Case <code>EXTRACTION</code> <code>spans</code> <code>List[Span]</code> Find text spans (untyped) <code>NER</code> <code>entities</code> <code>List[Entity]</code> Find typed entities with labels <code>CLASSIFICATION</code> <code>label</code> <code>str</code> Classify text into categories <code>TRANSFORMATION</code> Custom <code>Dict</code> Extract structured fields"},{"location":"guide/task-types/#extraction","title":"Extraction","text":"<p>Extraction finds text spans without type labels. Each span has <code>text</code>, <code>start</code>, and <code>end</code> fields.</p> <pre><code>task = Task(\n    name=\"Date Extraction\",\n    description=\"Extract date mentions from text\",\n    input_schema={\"text\": \"str\"},\n    output_schema={\"spans\": \"List[Span]\"},\n    type=TaskType.EXTRACTION,\n)\n</code></pre> <p>Output format: <pre><code>{\n  \"spans\": [\n    {\"text\": \"January 2024\", \"start\": 10, \"end\": 22}\n  ]\n}\n</code></pre></p>"},{"location":"guide/task-types/#matching-modes","title":"Matching Modes","text":"<p>For extraction evaluation, you can choose how spans are compared:</p> <pre><code>task = Task(\n    ...,\n    type=TaskType.EXTRACTION,\n    matching_mode=\"text\",   # Compare by span text only (default)\n    # matching_mode=\"exact\",  # Compare by text + start/end offsets\n)\n</code></pre>"},{"location":"guide/task-types/#ner-named-entity-recognition","title":"NER (Named Entity Recognition)","text":"<p>NER extracts typed entities. Each entity has <code>text</code>, <code>start</code>, <code>end</code>, and <code>type</code> fields.</p> <pre><code>from pydantic import BaseModel\nfrom typing import List, Literal\n\nclass Entity(BaseModel):\n    text: str\n    start: int\n    end: int\n    type: Literal[\"PERSON\", \"ORG\", \"LOCATION\"]\n\nclass NEROutput(BaseModel):\n    entities: List[Entity]\n\ntask = Task(\n    name=\"NER\",\n    description=\"Extract named entities\",\n    input_schema={\"text\": \"str\"},\n    output_schema=NEROutput,\n    type=TaskType.NER,\n)\n</code></pre> <p>Output format: <pre><code>{\n  \"entities\": [\n    {\"text\": \"Alice\", \"start\": 0, \"end\": 5, \"type\": \"PERSON\"},\n    {\"text\": \"Acme Corp\", \"start\": 15, \"end\": 24, \"type\": \"ORG\"}\n  ]\n}\n</code></pre></p> <p>Pydantic schemas</p> <p>Using a Pydantic model with <code>Literal</code> type fields lets RuleChef automatically discover valid labels and validate outputs at runtime.</p>"},{"location":"guide/task-types/#classification","title":"Classification","text":"<p>Classification assigns a single label to each input.</p> <pre><code>task = Task(\n    name=\"Sentiment\",\n    description=\"Classify text sentiment\",\n    input_schema={\"text\": \"str\"},\n    output_schema={\"label\": \"str\"},\n    type=TaskType.CLASSIFICATION,\n    text_field=\"text\",\n)\n</code></pre> <p>Output format: <pre><code>{\"label\": \"positive\"}\n</code></pre></p> <p>Classification matching is case-insensitive and strips whitespace.</p>"},{"location":"guide/task-types/#transformation","title":"Transformation","text":"<p>Transformation extracts arbitrary structured fields. The output schema defines the target shape.</p> <pre><code>task = Task(\n    name=\"Contact Parser\",\n    description=\"Extract name and email from text\",\n    input_schema={\"text\": \"str\"},\n    output_schema={\"name\": \"str\", \"email\": \"str\"},\n    type=TaskType.TRANSFORMATION,\n)\n</code></pre> <p>Output format: <pre><code>{\"name\": \"Alice Smith\", \"email\": \"alice@example.com\"}\n</code></pre></p>"},{"location":"guide/task-types/#input-schema-and-text-field","title":"Input Schema and Text Field","text":""},{"location":"guide/task-types/#multi-field-inputs","title":"Multi-Field Inputs","text":"<p>Tasks can have multiple input fields:</p> <pre><code>task = Task(\n    name=\"Q&amp;A\",\n    input_schema={\"question\": \"str\", \"context\": \"str\"},\n    output_schema={\"spans\": \"List[Span]\"},\n    type=TaskType.EXTRACTION,\n    text_field=\"context\",  # Regex/spaCy rules match against this field\n)\n</code></pre>"},{"location":"guide/task-types/#text-field-selection","title":"Text Field Selection","text":"<p>By default, regex and spaCy rules match against the longest string input field. Use <code>text_field</code> to specify which field to use:</p> <pre><code>task = Task(\n    ...,\n    text_field=\"context\",  # Explicit: use \"context\" field\n)\n</code></pre>"},{"location":"guide/task-types/#rule-formats","title":"Rule Formats","text":"<p>Rules can be generated in three formats:</p> Format Best For Speed <code>RuleFormat.REGEX</code> Keyword patterns, structured text Fastest <code>RuleFormat.CODE</code> Complex logic, multi-field extraction Fast <code>RuleFormat.SPACY</code> Linguistic patterns (POS, dependency) Moderate <pre><code>from rulechef import RuleFormat\n\n# Restrict to regex only (fastest, most portable)\nchef = RuleChef(task, client, allowed_formats=[RuleFormat.REGEX])\n\n# Allow code rules for complex logic\nchef = RuleChef(task, client, allowed_formats=[RuleFormat.CODE])\n\n# All formats\nchef = RuleChef(task, client, allowed_formats=[RuleFormat.REGEX, RuleFormat.CODE, RuleFormat.SPACY])\n</code></pre>"}]}